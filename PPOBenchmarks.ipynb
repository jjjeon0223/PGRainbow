{"cells":[{"cell_type":"code","source":["import torch\n","torch.__version__"],"metadata":{"id":"FeCLnWk0V-DE","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torchaudio\n","torchaudio.__version__"],"metadata":{"id":"afXBpqUWYVhb"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yMtwmfknVXTm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1721352663630,"user_tz":-540,"elapsed":20063,"user":{"displayName":"­전우재 / 학생 / 자유전공학부","userId":"15209413021938568538"}},"outputId":"0d5b5844-3d2d-4055-d250-8080a20b31b4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LSuvv358WdmB"},"outputs":[],"source":["!pip install gymnasium[atari]\n","!pip install gymnasium[accept-rom-license]\n","!pip install stable-baselines3[extra]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7j-6YRHxWuTR"},"outputs":[],"source":["!pip install numpngw\n","!pip install omegaconf\n","!pip install envpool"]},{"cell_type":"code","source":["from dataclasses import dataclass\n"],"metadata":{"id":"YPI3vCzeLFPB"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KZzZ2WHfWxhE"},"outputs":[],"source":["# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/ppo/#ppo_atari_envpoolpy\n","import os\n","import random\n","import time\n","from collections import deque\n","from dataclasses import dataclass\n","import math\n","from gymnasium.wrappers import AtariPreprocessing\n","from gymnasium.wrappers import FrameStack\n","from torch.autograd import Variable\n","\n","import envpool\n","import gym\n","\n","import numpy as np\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import tqdm\n","from torch.distributions.categorical import Categorical\n","from torch.utils.tensorboard import SummaryWriter"]},{"cell_type":"markdown","metadata":{"id":"ANBUy1TzlxN4"},"source":["## PPO"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CQqF_b5AXkRe"},"outputs":[],"source":["@dataclass\n","class Args:\n","    exp_name: str = \"iqn_ppo_run1\"\n","    \"\"\"the name of this experiment\"\"\"\n","    seed: int = 1\n","    \"\"\"seed of the experiment\"\"\"\n","    torch_deterministic: bool = True\n","    \"\"\"if toggled, `torch.backends.cudnn.deterministic=False`\"\"\"\n","    cuda: bool = True\n","    \"\"\"if toggled, cuda will be enabled by default\"\"\"\n","    track: bool = False\n","    \"\"\"if toggled, this experiment will be tracked with Weights and Biases\"\"\"\n","    wandb_project_name: str = \"cleanRL\"\n","    \"\"\"the wandb's project name\"\"\"\n","    wandb_entity: str = None\n","    \"\"\"the entity (team) of wandb's project\"\"\"\n","    capture_video: bool = False\n","    \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n","\n","    # Algorithm specific arguments\n","    env_id: str = \"SpaceInvaders-v5\"\n","    \"\"\"the id of the environment\"\"\"\n","    total_timesteps: int = 1000000\n","    \"\"\"total timesteps of the experiments\"\"\"\n","    learning_rate: float = 2.5e-4\n","    \"\"\"the learning rate of the optimizer\"\"\"\n","    num_envs: int = 8\n","    \"\"\"the number of parallel game environments\"\"\"\n","    num_steps: int = 128\n","    \"\"\"the number of steps to run in each environment per policy rollout\"\"\"\n","    anneal_lr: bool = True\n","    \"\"\"Toggle learning rate annealing for policy and value networks\"\"\"\n","    gamma: float = 0.99\n","    \"\"\"the discount factor gamma\"\"\"\n","    gae_lambda: float = 0.95\n","    \"\"\"the lambda for the general advantage estimation\"\"\"\n","    num_minibatches: int = 4\n","    \"\"\"the number of mini-batches\"\"\"\n","    update_epochs: int = 4\n","    \"\"\"the K epochs to update the policy\"\"\"\n","    norm_adv: bool = True\n","    \"\"\"Toggles advantages normalization\"\"\"\n","    clip_coef: float = 0.1\n","    \"\"\"the surrogate clipping coefficient\"\"\"\n","    clip_vloss: bool = True\n","    \"\"\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\"\"\"\n","    ent_coef: float = 0.01\n","    \"\"\"coefficient of the entropy\"\"\"\n","    vf_coef: float = 0.5\n","    \"\"\"coefficient of the value function\"\"\"\n","    max_grad_norm: float = 0.5\n","    \"\"\"the maximum norm for the gradient clipping\"\"\"\n","    target_kl: float = None\n","    \"\"\"the target KL divergence threshold\"\"\""]},{"cell_type":"markdown","metadata":{"id":"eSxMzLcUmx0t"},"source":["## Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gmRKxx8KV12d"},"outputs":[],"source":["class RecordEpisodeStatistics(gym.Wrapper):\n","    def __init__(self, env, deque_size=100):\n","        super().__init__(env)\n","        self.num_envs = getattr(env, \"num_envs\", 1)\n","        self.episode_returns = None\n","        self.episode_lengths = None\n","\n","    def reset(self, **kwargs):\n","        observations = super().reset(**kwargs)\n","        self.episode_returns = np.zeros(self.num_envs, dtype=np.float32)\n","        self.episode_lengths = np.zeros(self.num_envs, dtype=np.int32)\n","        self.lives = np.zeros(self.num_envs, dtype=np.int32)\n","        self.returned_episode_returns = np.zeros(self.num_envs, dtype=np.float32)\n","        self.returned_episode_lengths = np.zeros(self.num_envs, dtype=np.int32)\n","        return observations\n","\n","    def step(self, action):\n","        observations, rewards, dones, infos = super().step(action)\n","        self.episode_returns += infos[\"reward\"]\n","        self.episode_lengths += 1\n","        self.returned_episode_returns[:] = self.episode_returns\n","        self.returned_episode_lengths[:] = self.episode_lengths\n","        self.episode_returns *= 1 - infos[\"terminated\"]\n","        self.episode_lengths *= 1 - infos[\"terminated\"]\n","        infos[\"r\"] = self.returned_episode_returns\n","        infos[\"l\"] = self.returned_episode_lengths\n","        return (\n","            observations,\n","            rewards,\n","            dones,\n","            infos,\n","        )\n","\n","\n","def layer_init(layer, std=np.sqrt(2), bias_const=0.0):\n","    torch.nn.init.orthogonal_(layer.weight, std)\n","    torch.nn.init.constant_(layer.bias, bias_const)\n","    return layer"]},{"cell_type":"markdown","metadata":{"id":"PDbjQZcSmzVb"},"source":["## Agent Networks"]},{"cell_type":"markdown","metadata":{"id":"FCtYcVR9Ooat"},"source":["### PPO Agent"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BEHVMNF5parn"},"outputs":[],"source":["class Agent(nn.Module):\n","    def __init__(self, envs):\n","        super().__init__()\n","        self.network = nn.Sequential(\n","            layer_init(nn.Conv2d(4, 32, 8, stride=4)),\n","            nn.ReLU(),\n","            layer_init(nn.Conv2d(32, 64, 4, stride=2)),\n","            nn.ReLU(),\n","            layer_init(nn.Conv2d(64, 64, 3, stride=1)),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            layer_init(nn.Linear(64 * 7 * 7, 512)),\n","            nn.ReLU(),\n","        )\n","        self.actor = layer_init(nn.Linear(512, envs.single_action_space.n), std=0.01)\n","        self.critic = layer_init(nn.Linear(512, 1), std=1)\n","\n","    def get_value(self, x):\n","        return self.critic(self.network(x / 255.0))\n","\n","    def get_action_and_value(self, x, action=None):\n","        hidden = self.network(x / 255.0)\n","        logits = self.actor(hidden)\n","        probs = Categorical(logits=logits)\n","        if action is None:\n","            action = probs.sample()\n","        return action, probs.log_prob(action), probs.entropy(), self.critic(hidden)"]},{"cell_type":"markdown","metadata":{"id":"ronYepV6ngOg"},"source":["### Replace Value with Q"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Jr9G7ZROnmv1"},"outputs":[],"source":["class ReplaceValueQuantileAgent(nn.Module):\n","    def __init__(self, action_dim, N):\n","        super().__init__()\n","\n","        self.num_actions = action_dim\n","        self.N = N\n","\n","        self.network = nn.Sequential(\n","            torch.nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=0),\n","            nn.ReLU(),\n","            torch.nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n","            nn.ReLU(),\n","            torch.nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(64*7*7,512),\n","            nn.ReLU(),\n","        )\n","\n","        self.actor = nn.Linear(512, action_dim)\n","        self.critic = nn.Linear(512, 1)\n","        self.delta = 1.0\n","        self.distillation_network = QDistillationNetwork(action_dim, self.N)\n","\n","    def forward(self, x, q, action=None):\n","        #print(f\"x shape: {x.shape}\")\n","        state_embeddings = self.network(x / 255.0) # [batch_size, 512]\n","\n","        action_embeddings = self.actor(state_embeddings)\n","        dist = Categorical(logits=action_embeddings)\n","\n","        #critic_out = self.critic(state_embeddings)\n","\n","        #value_embeddings = self.distillation_network(critic_out.unsqueeze(-1), q.unsqueeze(-1))\n","        if action is None:\n","            action = dist.sample()\n","        return action, dist.log_prob(action), dist.entropy(), q.mean(dim=-1, keepdim=True)"]},{"cell_type":"markdown","metadata":{"id":"kH0QQ3XZ5tM_"},"source":["### Simple Embed"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uH2Sen8f5sgL"},"outputs":[],"source":["class SimpleEmbedAgent(nn.Module):\n","    def __init__(self, action_dim, N):\n","        super().__init__()\n","\n","        self.num_actions = action_dim\n","        self.N = N\n","\n","        self.network = nn.Sequential(\n","            torch.nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=0),\n","            nn.ReLU(),\n","            torch.nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n","            nn.ReLU(),\n","            torch.nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(64*7*7,512),\n","            nn.ReLU(),\n","        )\n","\n","        self.actor = nn.Linear(512, action_dim)\n","        self.critic = nn.Linear(512, 1)\n","        self.delta = 1.0\n","        self.distillation_network = QDistillationNetwork(action_dim, self.N)\n","\n","    def forward(self, x, q, action=None):\n","        #print(f\"x shape: {x.shape} and q shape: {q.shape}\")\n","        state_embeddings = self.network(x / 255.0) # [batch_size, 512]\n","\n","        action_embeddings = self.actor(state_embeddings)\n","        dist = Categorical(logits=action_embeddings)\n","\n","        critic_out = self.critic(state_embeddings)\n","        quantile_mean = torch.mean(q, dim=1).unsqueeze(-1)\n","        #print(f\"critic_out shape: {critic_out.shape} and q shape: {quantile_mean.shape}\")\n","        simple_embed = (critic_out + quantile_mean)/2\n","        #print(f\"simple embed shape: {simple_embed.shape}\")\n","        #raise ValueError(\"Training Stopped\")\n","\n","        #value_embeddings = self.distillation_network(critic_out.unsqueeze(-1), q.unsqueeze(-1))\n","        if action is None:\n","            action = dist.sample()\n","        return action, dist.log_prob(action), dist.entropy(), simple_embed"]},{"cell_type":"markdown","source":["### Attention"],"metadata":{"id":"7JyB_M5BoZTF"}},{"cell_type":"code","source":["class PositionalEncoding(nn.Module):\n","    \"Implement the PE function.\"\n","    def __init__(self, d_model, dropout, max_len=5000):\n","        super(PositionalEncoding, self).__init__()\n","        self.dropout = nn.Dropout(p=dropout)\n","\n","        # Compute the positional encodings once in log space.\n","        pe = torch.zeros(max_len, d_model)\n","        #print(f\"pe shape: {pe.shape}\")\n","        position = torch.arange(0, max_len).unsqueeze(1)\n","        #print(f\"position shape: {position.shape}\")\n","        div_term = torch.exp(torch.arange(0, d_model, 2) *\n","                             -(math.log(10000.0) / d_model))\n","        #print(f\"div_term shape: {div_term.shape}\")\n","        pe[:, 0::2] = torch.sin(position * div_term)\n","        pe[:, 1::2] = torch.cos(position * div_term)\n","        pe = pe.unsqueeze(0)\n","        #print(f\"pe shape: {pe.shape}\")\n","        self.register_buffer('pe', pe)\n","\n","    def forward(self, x):\n","        x = x.unsqueeze(-1)\n","        #print(f\"x shape: {x.shape}\")\n","        pos = Variable(self.pe[:, :x.size(-2)], requires_grad=False)\n","        #print(f\"pos shape: {pos.shape}\")\n","        x = x + pos\n","        return self.dropout(x)"],"metadata":{"id":"ss6bAbfZs-2k"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ol35cOcmoZTG"},"outputs":[],"source":["class AttentionQDistillationNetwork(nn.Module):\n","  def __init__(self, action_dim, N):\n","    super().__init__()\n","    self.action_dim = action_dim\n","    self.cat_size = action_dim + 1\n","    self.network = nn.Sequential(\n","        nn.Linear(512, 512//2),\n","        nn.ReLU(),\n","        nn.Linear(512//2, 1),\n","        nn.ReLU()\n","    )\n","    self.positional_encoding = PositionalEncoding(d_model=512, dropout=0.1)\n","    self.encoder = encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)\n","    self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)\n","\n","  def forward(self, embeddings, quantiles, scale=1.0):\n","    # q shape: [batch_size, action_dim]\n","    # embeddings shape: [batch_size, action_dim]\n","    batch_size = embeddings.shape[0]\n","    #print(f\"_embed: {embeddings.shape} and quantile: {quantiles.shape}\")\n","    assert embeddings.shape[0] == quantiles.shape[0]\n","\n","    scaled_quantiles = quantiles * scale\n","\n","    cat = torch.cat((embeddings, scaled_quantiles), dim=-1)\n","    pos_cat = self.positional_encoding(cat)\n","    encoder_out = self.transformer_encoder(pos_cat)\n","\n","    out = self.network(encoder_out)\n","    #print(f\"out shaep: {out.shape}\")\n","    #raise ValueError(\"Stopped\")\n","\n","    return out.mean(dim=1, keepdim=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_oYCx3iwoZTG"},"outputs":[],"source":["class AttentionAgentCriticDistillation(nn.Module):\n","    def __init__(self, action_dim, N):\n","        super().__init__()\n","\n","        self.num_actions = action_dim\n","        self.N = N\n","\n","        self.network = nn.Sequential(\n","            torch.nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=0),\n","            nn.ReLU(),\n","            torch.nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n","            nn.ReLU(),\n","            torch.nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(64*7*7,512),\n","            nn.ReLU(),\n","        )\n","\n","        self.actor = nn.Linear(512, action_dim)\n","        self.critic = nn.Linear(512, 1)\n","        self.delta = 1.0\n","        self.distillation_network = AttentionQDistillationNetwork(action_dim, self.N)\n","\n","    def forward(self, x, q, action=None):\n","        #print(f\"x shape: {x.shape}\")\n","        state_embeddings = self.network(x / 255.0) # [batch_size, 512]\n","\n","        action_embeddings = self.actor(state_embeddings)\n","        dist = Categorical(logits=action_embeddings)\n","\n","        critic_out = self.critic(state_embeddings)\n","\n","        value_embeddings = self.distillation_network(critic_out, q)\n","        if action is None:\n","            action = dist.sample()\n","        return action, dist.log_prob(action), dist.entropy(), value_embeddings"]},{"cell_type":"markdown","source":["### Concat"],"metadata":{"id":"7lZNLqzsKUIv"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"CQCaNS_dKYd6"},"outputs":[],"source":["class ConcatQDistillationNetwork(nn.Module):\n","  def __init__(self, action_dim, N):\n","    super().__init__()\n","    self.action_dim = action_dim\n","    self.cat_size = action_dim + 1\n","    self.network = nn.Sequential(\n","        nn.Linear(self.cat_size, self.cat_size*2),\n","        nn.ReLU(),\n","        nn.Linear(self.cat_size*2, self.cat_size),\n","        nn.ReLU()\n","    )\n","\n","  def forward(self, embeddings, quantiles, scale=1.0):\n","    # q shape: [batch_size, action_dim]\n","    # embeddings shape: [batch_size, action_dim]\n","    batch_size = embeddings.shape[0]\n","    #print(f\"_embed: {embeddings.shape} and quantile: {quantiles.shape}\")\n","    assert embeddings.shape[0] == quantiles.shape[0]\n","\n","    scaled_quantiles = quantiles * scale\n","\n","    out = (embeddings * scaled_quantiles)\n","    cat = torch.cat((embeddings, scaled_quantiles), dim=-1)\n","    #print(f\"cat shape: {cat.shape}\")\n","\n","\n","    assert out.shape == (batch_size, self.action_dim), f\"out shape: {out.shape}\"\n","    out = self.network(cat)\n","    #print(f\"out shaep: {out.shape}\")\n","    #raise ValueError(\"Stopped\")\n","\n","    return out.mean(dim=1, keepdim=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PrFuZEvbKYd7"},"outputs":[],"source":["class ConcatAgentCriticDistillation(nn.Module):\n","    def __init__(self, action_dim, N):\n","        super().__init__()\n","\n","        self.num_actions = action_dim\n","        self.N = N\n","\n","        self.network = nn.Sequential(\n","            torch.nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=0),\n","            nn.ReLU(),\n","            torch.nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n","            nn.ReLU(),\n","            torch.nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(64*7*7,512),\n","            nn.ReLU(),\n","        )\n","\n","        self.actor = nn.Linear(512, action_dim)\n","        self.critic = nn.Linear(512, 1)\n","        self.delta = 1.0\n","        self.distillation_network = ConcatQDistillationNetwork(action_dim, self.N)\n","\n","    def forward(self, x, q, action=None):\n","        #print(f\"x shape: {x.shape}\")\n","        state_embeddings = self.network(x / 255.0) # [batch_size, 512]\n","\n","        action_embeddings = self.actor(state_embeddings)\n","        dist = Categorical(logits=action_embeddings)\n","\n","        critic_out = self.critic(state_embeddings)\n","\n","        value_embeddings = self.distillation_network(critic_out, q)\n","        if action is None:\n","            action = dist.sample()\n","        return action, dist.log_prob(action), dist.entropy(), value_embeddings"]},{"cell_type":"markdown","metadata":{"id":"bkbf6U-Nm8ul"},"source":["### Bilinear Weights"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YBlER38pm8uv"},"outputs":[],"source":["class BilinearQDistillationNetwork(nn.Module):\n","  def __init__(self, action_dim, N):\n","    super().__init__()\n","    self.action_dim = action_dim\n","    self.network = nn.Sequential(\n","        nn.Linear(1, action_dim),\n","        nn.ReLU(),\n","        nn.Linear(action_dim, 1),\n","        nn.ReLU()\n","    )\n","    self.bilinear = nn.Bilinear(6,1,1)\n","\n","  def forward(self, embeddings, quantiles, scale=1.0):\n","    # q shape: [batch_size, action_dim]\n","    # embeddings shape: [batch_size, action_dim]\n","    batch_size = embeddings.shape[0]\n","    #print(f\"_embed: {embeddings.shape} and quantile: {quantiles.shape}\")\n","    assert embeddings.shape[0] == quantiles.shape[0]\n","\n","    out = self.bilinear(quantiles, embeddings)\n","    #print(f\"out shape: {out.shape}\")\n","    #raise ValueError(\"Stopped\")\n","\n","    #out = self.network(out)\n","    #print(f\"out shape: {out.shape}\")\n","    #raise ValueError(\"Stopped\")\n","\n","    return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2OqR5Ghnm8uv"},"outputs":[],"source":["class BilinearAgentCriticDistillation(nn.Module):\n","    def __init__(self, action_dim, N):\n","        super().__init__()\n","\n","        self.num_actions = action_dim\n","        self.N = N\n","\n","        self.network = nn.Sequential(\n","            torch.nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=0),\n","            nn.ReLU(),\n","            torch.nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n","            nn.ReLU(),\n","            torch.nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(64*7*7,512),\n","            nn.ReLU(),\n","        )\n","\n","        self.actor = nn.Linear(512, action_dim)\n","        self.critic = nn.Linear(512, 1)\n","        self.delta = 1.0\n","        self.distillation_network = BilinearQDistillationNetwork(action_dim, self.N)\n","\n","    def forward(self, x, q, action=None):\n","        #print(f\"x shape: {x.shape}\")\n","        state_embeddings = self.network(x / 255.0) # [batch_size, 512]\n","\n","        action_embeddings = self.actor(state_embeddings)\n","        dist = Categorical(logits=action_embeddings)\n","\n","        critic_out = self.critic(state_embeddings)\n","\n","        value_embeddings = self.distillation_network(critic_out, q)\n","        if action is None:\n","            action = dist.sample()\n","        return action, dist.log_prob(action), dist.entropy(), value_embeddings"]},{"cell_type":"markdown","metadata":{"id":"o-t0IDtzfXmN"},"source":["### Learnable Weights"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XADaCyJffXmN"},"outputs":[],"source":["class LearnableWeightsQDistillationNetwork(nn.Module):\n","  def __init__(self, action_dim, N):\n","    super().__init__()\n","    self.action_dim = action_dim\n","    self.network = nn.Sequential(\n","        nn.Linear(1, action_dim),\n","        nn.ReLU(),\n","        nn.Linear(action_dim, 1),\n","        nn.ReLU()\n","    )\n","    self.weights = torch.nn.Parameter(torch.rand(6))\n","    self.bias = torch.nn.Parameter(torch.rand(1))\n","\n","  def forward(self, embeddings, quantiles, scale=1.0):\n","    # q shape: [batch_size, action_dim]\n","    # embeddings shape: [batch_size, action_dim]\n","    batch_size = embeddings.shape[0]\n","    #print(f\"_embed: {embeddings.shape} and quantile: {quantiles.shape}\")\n","    assert embeddings.shape[0] == quantiles.shape[0]\n","\n","    scaled_quantiles = torch.mean(quantiles * self.weights, dim=-1).unsqueeze(-1)\n","    #print(f\"scaled_quantiles: {scaled_quantiles.shape}\")\n","\n","    scaled_embeddings = self.bias * embeddings\n","    #print(f\"scaled_embeddings: {scaled_embeddings.shape}\")\n","\n","    out = scaled_embeddings + scaled_quantiles\n","    #print(f\"out shape: {out.shape}\")\n","    #raise ValueError(\"Stopped\")\n","\n","    out = self.network(out)\n","    #print(f\"out shape: {out.shape}\")\n","    #raise ValueError(\"Stopped\")\n","\n","    return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RAEDSkWyfXmO"},"outputs":[],"source":["class LearnableWeightsAgentCriticDistillation(nn.Module):\n","    def __init__(self, action_dim, N):\n","        super().__init__()\n","\n","        self.num_actions = action_dim\n","        self.N = N\n","\n","        self.network = nn.Sequential(\n","            torch.nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=0),\n","            nn.ReLU(),\n","            torch.nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n","            nn.ReLU(),\n","            torch.nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(64*7*7,512),\n","            nn.ReLU(),\n","        )\n","\n","        self.actor = nn.Linear(512, action_dim)\n","        self.critic = nn.Linear(512, 1)\n","        self.delta = 1.0\n","        self.distillation_network = LearnableWeightsQDistillationNetwork(action_dim, self.N)\n","\n","    def forward(self, x, q, action=None):\n","        #print(f\"x shape: {x.shape}\")\n","        state_embeddings = self.network(x / 255.0) # [batch_size, 512]\n","\n","        action_embeddings = self.actor(state_embeddings)\n","        dist = Categorical(logits=action_embeddings)\n","\n","        critic_out = self.critic(state_embeddings)\n","\n","        value_embeddings = self.distillation_network(critic_out, q)\n","        if action is None:\n","            action = dist.sample()\n","        return action, dist.log_prob(action), dist.entropy(), value_embeddings"]},{"cell_type":"markdown","metadata":{"id":"Xj1nwxG25kSX"},"source":["### ScaledQDistillation\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zlPbo0f95kSs"},"outputs":[],"source":["class ScaledQDistillationNetwork(nn.Module):\n","  def __init__(self, action_dim, N):\n","    super().__init__()\n","    self.action_dim = action_dim\n","    self.network = nn.Sequential(\n","        nn.Linear(action_dim, action_dim),\n","        nn.ReLU(),\n","        nn.Linear(action_dim, action_dim),\n","        nn.ReLU()\n","    )\n","    self.scale = nn.Parameter(torch.rand(1))\n","\n","  def forward(self, embeddings, quantiles, scale=1.0):\n","    # q shape: [batch_size, action_dim]\n","    # embeddings shape: [batch_size, action_dim]\n","    batch_size = embeddings.shape[0]\n","    #print(f\"_embed: {embeddings.shape} and quantile: {quantiles.shape}\")\n","    assert embeddings.shape[0] == quantiles.shape[0]\n","\n","    scaled_quantiles = quantiles * self.scale\n","    #print(f\"scaled_quantiles: {scaled_quantiles.shape}\")\n","    out = (embeddings * scaled_quantiles)\n","    #print(f\"out: {out.shape}\")\n","\n","    out = self.network(out)\n","\n","    return out.mean(dim=1, keepdim=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3UyECgZ55kSs"},"outputs":[],"source":["class ScaledAgentCriticDistillation(nn.Module):\n","    def __init__(self, action_dim, N):\n","        super().__init__()\n","\n","        self.num_actions = action_dim\n","        self.N = N\n","\n","        self.network = nn.Sequential(\n","            torch.nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=0),\n","            nn.ReLU(),\n","            torch.nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n","            nn.ReLU(),\n","            torch.nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(64*7*7,512),\n","            nn.ReLU(),\n","        )\n","\n","        self.actor = nn.Linear(512, action_dim)\n","        self.critic = nn.Linear(512, 1)\n","        self.delta = 1.0\n","        self.distillation_network = ScaledQDistillationNetwork(action_dim, self.N)\n","\n","    def forward(self, x, q, action=None):\n","        #print(f\"x shape: {x.shape}\")\n","        state_embeddings = self.network(x / 255.0) # [batch_size, 512]\n","\n","        action_embeddings = self.actor(state_embeddings)\n","        dist = Categorical(logits=action_embeddings)\n","\n","        critic_out = self.critic(state_embeddings)\n","\n","        value_embeddings = self.distillation_network(critic_out, q)\n","        if action is None:\n","            action = dist.sample()\n","        return action, dist.log_prob(action), dist.entropy(), value_embeddings"]},{"cell_type":"markdown","metadata":{"id":"McYJ99a4OqPr"},"source":["### QDistillation (Best Results)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gswq2h7Awb_y"},"outputs":[],"source":["class QDistillationNetwork(nn.Module):\n","  def __init__(self, action_dim, N):\n","    super().__init__()\n","    self.action_dim = action_dim\n","    self.network = nn.Sequential(\n","        nn.Linear(action_dim, action_dim),\n","        nn.ReLU(),\n","        nn.Linear(action_dim, action_dim),\n","        nn.ReLU()\n","    )\n","\n","  def forward(self, embeddings, quantiles, scale=1.0):\n","    # q shape: [batch_size, action_dim]\n","    # embeddings shape: [batch_size, action_dim]\n","    batch_size = embeddings.shape[0]\n","    #print(f\"_embed: {embeddings.shape} and quantile: {quantiles.shape}\")\n","    assert embeddings.shape[0] == quantiles.shape[0]\n","\n","    scaled_quantiles = quantiles * scale\n","\n","    out = (embeddings * scaled_quantiles)\n","    out = out.squeeze(-1)\n","\n","    assert out.shape == (batch_size, self.action_dim), f\"out shape: {out.shape}\"\n","    out = self.network(out)\n","\n","    return out.mean(dim=1, keepdim=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_43sE0qkiTMl"},"outputs":[],"source":["class AgentCriticDistillation(nn.Module):\n","    def __init__(self, action_dim, N):\n","        super().__init__()\n","\n","        self.num_actions = action_dim\n","        self.N = N\n","\n","        self.network = nn.Sequential(\n","            torch.nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=0),\n","            nn.ReLU(),\n","            torch.nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n","            nn.ReLU(),\n","            torch.nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(64*7*7,512),\n","            nn.ReLU(),\n","        )\n","\n","        self.actor = nn.Linear(512, action_dim)\n","        self.critic = nn.Linear(512, 1)\n","        self.delta = 1.0\n","        self.distillation_network = QDistillationNetwork(action_dim, self.N)\n","\n","    def forward(self, x, q, action=None):\n","        #print(f\"x shape: {x.shape}\")\n","        state_embeddings = self.network(x / 255.0) # [batch_size, 512]\n","\n","        action_embeddings = self.actor(state_embeddings)\n","        dist = Categorical(logits=action_embeddings)\n","\n","        critic_out = self.critic(state_embeddings)\n","\n","        value_embeddings = self.distillation_network(critic_out.unsqueeze(-1), q.unsqueeze(-1))\n","        if action is None:\n","            action = dist.sample()\n","        return action, dist.log_prob(action), dist.entropy(), value_embeddings"]},{"cell_type":"markdown","metadata":{"id":"NwwauBwS3u8y"},"source":["### TD Method"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s28USGLr3u8y"},"outputs":[],"source":["class TDDistillationNetwork(nn.Module):\n","  def __init__(self, action_dim, N):\n","    super().__init__()\n","    self.action_dim = action_dim\n","    self.network = nn.Sequential(\n","        nn.Linear(action_dim, action_dim),\n","        nn.ELU()\n","    )\n","\n","  def forward(self, embeddings, quantiles, action_probs):\n","    # q shape: [batch_size, action_dim]\n","    # embeddings shape: [batch_size, action_dim]\n","    batch_size = embeddings.shape[0]\n","    # print(f\"_embed: {embeddings.shape} and quantile: {quantiles.shape}\")\n","    assert embeddings.shape[0] == quantiles.shape[0], embeddings.shape\n","    assert quantiles.shape == action_probs.shape, f\"quantiles: {quantiles.shape}, action: {action_probs.shape}\"\n","\n","    out = embeddings + self.network((action_probs * quantiles) - embeddings)\n","\n","    assert out.shape == (batch_size, self.action_dim), f\"out shape: {out.shape}\"\n","    # out = self.network(out)\n","\n","    return out.mean(dim=1, keepdim=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gl7n2FAZ3u8y"},"outputs":[],"source":["class AgentCriticTDDistillation(nn.Module):\n","    def __init__(self, action_dim, N):\n","        super().__init__()\n","\n","        self.num_actions = action_dim\n","        self.N = N\n","\n","        self.network = nn.Sequential(\n","            torch.nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=0),\n","            nn.ReLU(),\n","            torch.nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n","            nn.ReLU(),\n","            torch.nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(64*7*7,512),\n","            nn.ReLU(),\n","        )\n","\n","        self.actor = nn.Linear(512, action_dim)\n","        self.critic = nn.Linear(512, 1)\n","        self.delta = 1.0\n","        self.distillation_network = TDDistillationNetwork(action_dim, self.N)\n","\n","    def forward(self, x, q, action=None):\n","        # print(f\"x shape: {x.shape}\")\n","        state_embeddings = self.network(x / 255.0) # [batch_size, 512]\n","\n","        action_embeddings = self.actor(state_embeddings)\n","        dist = Categorical(logits=action_embeddings)\n","\n","        critic_out = self.critic(state_embeddings)\n","\n","        value_embeddings = self.distillation_network(critic_out, q, dist.probs)\n","        if action is None:\n","            action = dist.sample()\n","        return action, dist.log_prob(action), dist.entropy(), value_embeddings"]},{"cell_type":"markdown","metadata":{"id":"_yBjr0u6Os_f"},"source":["### SIngleHead Network Distillation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tFm_3oGcXxmM"},"outputs":[],"source":["class SingleHeadQDistillationNetwork(nn.Module):\n","  def __init__(self, action_dim, N):\n","    super().__init__()\n","    self.action_dim = action_dim\n","    self.network = nn.Sequential(\n","        nn.Linear(action_dim, action_dim),\n","        nn.ReLU(),\n","        nn.Linear(action_dim, 1),\n","        nn.ReLU()\n","    )\n","\n","  def forward(self, embeddings, quantiles):\n","    # q shape: [batch_size, action_dim]\n","    # embeddings shape: [batch_size, action_dim]\n","    batch_size = embeddings.shape[0]\n","    #print(f\"_embed: {embeddings.shape} and quantile: {quantiles.shape}\")\n","    assert embeddings.shape[0] == quantiles.shape[0]\n","\n","    out = (embeddings * quantiles)\n","    out = out.squeeze(-1)\n","\n","    assert out.shape == (batch_size, self.action_dim), f\"out shape: {out.shape}\"\n","    out = self.network(out)\n","\n","    return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JDQE0J3HXxmN"},"outputs":[],"source":["class SingleHeadAgentCriticDistillation(nn.Module):\n","    def __init__(self, action_dim, N):\n","        super().__init__()\n","\n","        self.num_actions = action_dim\n","        self.N = N\n","\n","        self.network = nn.Sequential(\n","            torch.nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=0),\n","            nn.ReLU(),\n","            torch.nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n","            nn.ReLU(),\n","            torch.nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(64*7*7,512),\n","            nn.ReLU(),\n","        )\n","\n","        self.actor = nn.Linear(512, action_dim)\n","        self.critic = nn.Linear(512, 1)\n","        self.delta = 1.0\n","        self.distillation_network = QDistillationNetwork(action_dim, self.N)\n","\n","    def AddNormalize(self, embeddings, distillation_embeddings):\n","        embeds = embeddings + self.delta * distillation_embeddings\n","        mean = embeds.mean(dim=-1, keepdim=True)\n","        std = embeds.std(dim=-1, keepdim=True)\n","        embeds = (embeds - mean) / (std + 1e-6)\n","        return embeds\n","\n","    def forward(self, x, q, action=None):\n","        #print(f\"x shape: {x.shape}\")\n","        state_embeddings = self.network(x / 255.0) # [batch_size, 512]\n","\n","        action_embeddings = self.actor(state_embeddings)\n","        dist = Categorical(logits=action_embeddings)\n","\n","        critic_out = self.critic(state_embeddings)\n","\n","        value_embeddings = self.distillation_network(critic_out.unsqueeze(-1), q.unsqueeze(-1))\n","        if action is None:\n","            action = dist.sample()\n","        return action, dist.log_prob(action), dist.entropy(), value_embeddings"]},{"cell_type":"markdown","metadata":{"id":"WGjxoTeMXxmF"},"source":["### SIngleHead Mean Distillation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fYvhpwnaOcD6"},"outputs":[],"source":["class AvgHeadAgentCriticDistillation(nn.Module):\n","    def __init__(self, action_dim, N):\n","        super().__init__()\n","\n","        self.num_actions = action_dim\n","        self.N = N\n","\n","        self.network = nn.Sequential(\n","            torch.nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=0),\n","            nn.ReLU(),\n","            torch.nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n","            nn.ReLU(),\n","            torch.nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(64*7*7,512),\n","            nn.ReLU(),\n","        )\n","\n","        self.actor = nn.Linear(512, action_dim)\n","        self.critic = nn.Linear(512, 1)\n","        self.delta = 1.0\n","        self.distillation_network = QDistillationNetwork(action_dim, self.N)\n","\n","    def AddNormalize(self, embeddings, distillation_embeddings):\n","        embeds = embeddings + self.delta * distillation_embeddings\n","        mean = embeds.mean(dim=-1, keepdim=True)\n","        std = embeds.std(dim=-1, keepdim=True)\n","        embeds = (embeds - mean) / (std + 1e-6)\n","        return embeds\n","\n","    def forward(self, x, q, action=None):\n","        #print(f\"x shape: {x.shape}\")\n","        state_embeddings = self.network(x / 255.0) # [batch_size, 512]\n","\n","        action_embeddings = self.actor(state_embeddings)\n","        dist = Categorical(logits=action_embeddings)\n","\n","        critic_out = self.critic(state_embeddings)\n","\n","        value_embeddings = (critic_out + q.mean(dim=-1, keepdim=True)) / 2\n","\n","        if action is None:\n","            action = dist.sample()\n","        return action, dist.log_prob(action), dist.entropy(), value_embeddings"]},{"cell_type":"markdown","metadata":{"id":"Fl0XHuGtOybY"},"source":["### Normalized Distillation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pakp05t2EnB9"},"outputs":[],"source":["class NormalizedQDistillationNetwork(nn.Module):\n","  def __init__(self, action_dim, N):\n","    super().__init__()\n","    self.action_dim = action_dim\n","    self.network = nn.Sequential(\n","        nn.Linear(action_dim, action_dim),\n","        nn.ReLU(),\n","        nn.Linear(action_dim, action_dim),\n","        nn.ReLU()\n","    )\n","\n","  def forward(self, embeddings, quantiles):\n","    # q shape: [batch_size, action_dim]\n","    # embeddings shape: [batch_size, action_dim]\n","    batch_size = embeddings.shape[0]\n","    #print(f\"_embed: {embeddings.shape} and quantile: {quantiles.shape}\")\n","    assert embeddings.shape[0] == quantiles.shape[0]\n","\n","    out = (embeddings * quantiles)\n","    out = out.squeeze(-1)\n","\n","    assert out.shape == (batch_size, self.action_dim), f\"out shape: {out.shape}\"\n","    out = self.network(out)\n","\n","    return out"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K-Db659ubWIX"},"outputs":[],"source":["class AgentCriticNormalizedDistillation(nn.Module):\n","    def __init__(self, action_dim, N):\n","        super().__init__()\n","\n","        self.num_actions = action_dim\n","        self.N = N\n","\n","        self.network = nn.Sequential(\n","            torch.nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=0),\n","            nn.ReLU(),\n","            torch.nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n","            nn.ReLU(),\n","            torch.nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(64*7*7,512),\n","            nn.ReLU(),\n","        )\n","\n","        self.actor = nn.Linear(512, action_dim)\n","        self.critic = nn.Linear(512, action_dim)\n","        self.delta = 1.0\n","        self.distillation_network = NormalizedQDistillationNetwork(action_dim, self.N)\n","\n","    def AddNormalize(self, embeddings, distillation_embeddings):\n","        embeds = embeddings + self.delta * distillation_embeddings\n","        mean = embeds.mean(dim=-1, keepdim=True)\n","\n","        std = embeds.std(dim=-1, keepdim=True)\n","\n","        embeds = (embeds - mean) / (std + 1e-6)\n","        return embeds\n","\n","    def forward(self, x, q, start_iqn=False, action=None):\n","        #print(f\"x shape: {x.shape}\")\n","        state_embeddings = self.network(x / 255.0) # [batch_size, 512]\n","\n","        #print(state_embeddings.shape)\n","        action_embeddings = self.actor(state_embeddings)\n","        #print(action_embeddings)\n","        dist = Categorical(logits=action_embeddings)\n","\n","        critic_out = self.critic(state_embeddings)\n","\n","        value_embeddings = self.distillation_network(critic_out.unsqueeze(-1), q.unsqueeze(-1))\n","\n","        value_embeddings = self.AddNormalize(critic_out, value_embeddings)\n","\n","        if action is None:\n","            action = dist.sample()\n","        return action, dist.log_prob(action), dist.entropy(), value_embeddings.mean(dim=-1, keepdim=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UngALk0fHXa9"},"outputs":[],"source":["\"\"\"class AgentCriticLagDistillation(nn.Module):\n","    def __init__(self, action_dim, N):\n","        super().__init__()\n","\n","        self.num_actions = action_dim\n","        self.N = N\n","\n","        self.network = nn.Sequential(\n","            torch.nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=0),\n","            nn.ReLU(),\n","            torch.nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n","            nn.ReLU(),\n","            torch.nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(64*7*7,512),\n","            nn.ReLU(),\n","        )\n","\n","        self.actor = nn.Linear(512, action_dim)\n","        self.critic = nn.Linear(512, action_dim)\n","        self.delta = 0.5\n","        self.distillation_network = QDistillationNetwork(action_dim, self.N)\n","\n","    def AddNormalize(self, action_embeddings, distillation_embeddings):\n","        embeds = action_embeddings + self.delta * distillation_embeddings\n","        mean = embeds.mean(dim=-1, keepdim=True)\n","        std = embeds.std(dim=-1, keepdim=True)\n","        embeds = (embeds - mean) / (std + 1e-6)\n","        return embeds\n","\n","    def forward(self, x, q, start_iqn=False, action=None):\n","        #print(f\"x shape: {q.shape}\")\n","        state_embeddings = self.network(x / 255.0) # [batch_size, 512]\n","\n","        action_embeddings = self.actor(state_embeddings)\n","        dist = Categorical(logits=action_embeddings)\n","\n","        value_embeddings = self.distillation_network(self.critic(state_embeddings), q).mean(dim=1, keepdim=True) if start_iqn else self.crtic(state_embeddings).mean(dim=1, keepdim=True)\n","        if action is None:\n","            action = dist.sample()\n","        return action, dist.log_prob(action), dist.entropy(), value_embeddings\"\"\""]},{"cell_type":"markdown","metadata":{"id":"7S-hGPqKuB4H"},"source":["### Lag Distillation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_WlceQmBuIBd"},"outputs":[],"source":["class LagQDistillationNetwork(nn.Module):\n","  def __init__(self, action_dim, N):\n","    super().__init__()\n","    self.action_dim = action_dim\n","    self.network = nn.Sequential(\n","        nn.Linear(action_dim, action_dim),\n","        nn.ReLU(),\n","        nn.Linear(action_dim, action_dim),\n","        nn.ReLU()\n","    )\n","\n","  def forward(self, embeddings, quantiles, start_iqn):\n","    # q shape: [batch_size, action_dim]\n","    # embeddings shape: [batch_size, action_dim]\n","    batch_size = embeddings.shape[0]\n","    #print(f\"_embed: {embeddings.shape} and quantile: {quantiles.shape}\")\n","    assert embeddings.shape[0] == quantiles.shape[0]\n","\n","    out = (embeddings * quantiles)\n","    out = out.squeeze(-1)\n","\n","    assert out.shape == (batch_size, self.action_dim), f\"out shape: {out.shape}\"\n","    out = self.network(out)\n","\n","    return out.mean(dim=1, keepdim=True) if start_iqn else embeddings.squeeze(-1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KT0F4fUPuIBe"},"outputs":[],"source":["class LagAgentCriticDistillation(nn.Module):\n","    def __init__(self, action_dim, N):\n","        super().__init__()\n","\n","        self.num_actions = action_dim\n","        self.N = N\n","\n","        self.network = nn.Sequential(\n","            torch.nn.Conv2d(4, 32, kernel_size=8, stride=4, padding=0),\n","            nn.ReLU(),\n","            torch.nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n","            nn.ReLU(),\n","            torch.nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n","            nn.ReLU(),\n","            nn.Flatten(),\n","            nn.Linear(64*7*7,512),\n","            nn.ReLU(),\n","        )\n","\n","        self.actor = nn.Linear(512, action_dim)\n","        self.critic = nn.Linear(512, 1)\n","        self.delta = 1.0\n","        self.distillation_network = LagQDistillationNetwork(action_dim, self.N)\n","\n","    def forward(self, x, q, action=None, start_iqn=False):\n","        #print(f\"x shape: {x.shape}\")\n","        state_embeddings = self.network(x / 255.0) # [batch_size, 512]\n","\n","        action_embeddings = self.actor(state_embeddings)\n","        dist = Categorical(logits=action_embeddings)\n","\n","        critic_out = self.critic(state_embeddings)\n","\n","        value_embeddings = self.distillation_network(critic_out.unsqueeze(-1), q.unsqueeze(-1), start_iqn)\n","        if action is None:\n","            action = dist.sample()\n","\n","        return action, dist.log_prob(action), dist.entropy(), value_embeddings"]},{"cell_type":"markdown","metadata":{"id":"Me3-jYDATLYN"},"source":["## IQN\n","[Code Link](https://github.com/toshikwa/fqf-iqn-qrdqn.pytorch)"]},{"cell_type":"markdown","metadata":{"id":"zdDQU6ygpbp8"},"source":["### IQN Utils"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vR2jS-0Kpc_s"},"outputs":[],"source":["def update_params(optim, loss, networks, retain_graph=False,\n","                  grad_cliping=None):\n","    optim.zero_grad()\n","    loss.backward(retain_graph=retain_graph)\n","    # Clip norms of gradients to stebilize training.\n","    if grad_cliping:\n","        for net in networks:\n","            torch.nn.utils.clip_grad_norm_(net.parameters(), grad_cliping)\n","    optim.step()\n","\n","\n","def disable_gradients(network):\n","    # Disable calculations of gradients.\n","    for param in network.parameters():\n","        param.requires_grad = False\n","\n","\n","def calculate_huber_loss(td_errors, kappa=1.0):\n","    return torch.where(\n","        td_errors.abs() <= kappa,\n","        0.5 * td_errors.pow(2),\n","        kappa * (td_errors.abs() - 0.5 * kappa))\n","\n","\n","def calculate_quantile_huber_loss(td_errors, taus, weights=None, kappa=1.0):\n","    assert not taus.requires_grad\n","    batch_size, N, N_dash = td_errors.shape\n","\n","    # Calculate huber loss element-wisely.\n","    element_wise_huber_loss = calculate_huber_loss(td_errors, kappa)\n","    assert element_wise_huber_loss.shape == (\n","        batch_size, N, N_dash)\n","\n","    # Calculate quantile huber loss element-wisely.\n","    element_wise_quantile_huber_loss = torch.abs(\n","        taus[..., None] - (td_errors.detach() < 0).float()\n","        ) * element_wise_huber_loss / kappa\n","    assert element_wise_quantile_huber_loss.shape == (\n","        batch_size, N, N_dash)\n","\n","    # Quantile huber loss.\n","    batch_quantile_huber_loss = element_wise_quantile_huber_loss.sum(\n","        dim=1).mean(dim=1, keepdim=True)\n","    assert batch_quantile_huber_loss.shape == (batch_size, 1)\n","\n","    if weights is not None:\n","        quantile_huber_loss = (batch_quantile_huber_loss * weights).mean()\n","    else:\n","        quantile_huber_loss = batch_quantile_huber_loss.mean()\n","\n","    return quantile_huber_loss\n","\n","\n","def evaluate_quantile_at_action(s_quantiles, actions):\n","    assert s_quantiles.shape[0] == actions.shape[0]\n","\n","    batch_size = s_quantiles.shape[0]\n","    N = s_quantiles.shape[1]\n","\n","    # Expand actions into (batch_size, N, 1).\n","    action_index = actions[..., None].expand(batch_size, N, 1)\n","\n","    # Calculate quantile values at specified actions.\n","    sa_quantiles = s_quantiles.gather(dim=2, index=action_index)\n","\n","    return sa_quantiles\n","\n","\n","class RunningMeanStats:\n","\n","    def __init__(self, n=10):\n","        self.n = n\n","        self.stats = deque(maxlen=n)\n","\n","    def append(self, x):\n","        self.stats.append(x)\n","\n","    def get(self):\n","        return np.mean(self.stats)\n","\n","\n","class LinearAnneaer:\n","\n","    def __init__(self, start_value, end_value, num_steps):\n","        assert num_steps > 0 and isinstance(num_steps, int)\n","\n","        self.steps = 0\n","        self.start_value = start_value\n","        self.end_value = end_value\n","        self.num_steps = num_steps\n","\n","        self.a = (self.end_value - self.start_value) / self.num_steps\n","        self.b = self.start_value\n","\n","    def step(self):\n","        self.steps = min(self.num_steps, self.steps + 1)\n","\n","    def get(self):\n","        assert 0 < self.steps <= self.num_steps\n","        return self.a * self.steps + self.b\n"]},{"cell_type":"markdown","metadata":{"id":"9VvQgBfjp5zD"},"source":["### Networks"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WfH40RBtp4aa"},"outputs":[],"source":["from copy import copy\n","import numpy as np\n","import torch\n","from torch import nn\n","import torch.nn.functional as F\n","\n","\n","def initialize_weights_xavier(m, gain=1.0):\n","    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","        torch.nn.init.xavier_uniform_(m.weight, gain=gain)\n","        if m.bias is not None:\n","            torch.nn.init.constant_(m.bias, 0)\n","\n","\n","def initialize_weights_he(m):\n","    if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):\n","        torch.nn.init.kaiming_uniform_(m.weight)\n","        if m.bias is not None:\n","            torch.nn.init.constant_(m.bias, 0)\n","\n","\n","class Flatten(nn.Module):\n","    def forward(self, x):\n","        return x.view(x.size(0), -1)\n","\n","\n","class DQNBase(nn.Module):\n","\n","    def __init__(self, num_channels, embedding_dim=7*7*64):\n","        super(DQNBase, self).__init__()\n","\n","        self.net = nn.Sequential(\n","            nn.Conv2d(num_channels, 32, kernel_size=8, stride=4, padding=0),\n","            nn.ReLU(),\n","            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=0),\n","            nn.ReLU(),\n","            nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=0),\n","            nn.ReLU(),\n","            Flatten(),\n","        ).apply(initialize_weights_he)\n","\n","        self.embedding_dim = embedding_dim\n","\n","    def forward(self, states):\n","        batch_size = states.shape[0]\n","\n","        # Calculate embeddings of states.\n","        state_embedding = self.net(states)\n","        assert state_embedding.shape == (batch_size, self.embedding_dim)\n","\n","        return state_embedding\n","\n","\n","class FractionProposalNetwork(nn.Module):\n","\n","    def __init__(self, N=32, embedding_dim=7*7*64):\n","        super(FractionProposalNetwork, self).__init__()\n","\n","        self.net = nn.Sequential(\n","            nn.Linear(embedding_dim, N)\n","        ).apply(lambda x: initialize_weights_xavier(x, gain=0.01))\n","\n","        self.N = N\n","        self.embedding_dim = embedding_dim\n","\n","    def forward(self, state_embeddings):\n","\n","        batch_size = state_embeddings.shape[0]\n","\n","        # Calculate (log of) probabilities q_i in the paper.\n","        log_probs = F.log_softmax(self.net(state_embeddings), dim=1)\n","        probs = log_probs.exp()\n","        assert probs.shape == (batch_size, self.N)\n","\n","        tau_0 = torch.zeros(\n","            (batch_size, 1), dtype=state_embeddings.dtype,\n","            device=state_embeddings.device)\n","        taus_1_N = torch.cumsum(probs, dim=1)\n","\n","        # Calculate \\tau_i (i=0,...,N).\n","        taus = torch.cat((tau_0, taus_1_N), dim=1)\n","        assert taus.shape == (batch_size, self.N+1)\n","\n","        # Calculate \\hat \\tau_i (i=0,...,N-1).\n","        tau_hats = (taus[:, :-1] + taus[:, 1:]).detach() / 2.\n","        assert tau_hats.shape == (batch_size, self.N)\n","\n","        # Calculate entropies of value distributions.\n","        entropies = -(log_probs * probs).sum(dim=-1, keepdim=True)\n","        assert entropies.shape == (batch_size, 1)\n","\n","        return taus, tau_hats, entropies\n","\n","\n","class CosineEmbeddingNetwork(nn.Module):\n","\n","    def __init__(self, num_cosines=64, embedding_dim=7*7*64, noisy_net=False):\n","        super(CosineEmbeddingNetwork, self).__init__()\n","        linear = NoisyLinear if noisy_net else nn.Linear\n","\n","        self.net = nn.Sequential(\n","            linear(num_cosines, embedding_dim),\n","            nn.ReLU()\n","        )\n","        self.num_cosines = num_cosines\n","        self.embedding_dim = embedding_dim\n","\n","    def forward(self, taus):\n","        batch_size = taus.shape[0]\n","        N = taus.shape[1]\n","\n","        # Calculate i * \\pi (i=1,...,N).\n","        i_pi = np.pi * torch.arange(\n","            start=1, end=self.num_cosines+1, dtype=taus.dtype,\n","            device=taus.device).view(1, 1, self.num_cosines)\n","\n","        # Calculate cos(i * \\pi * \\tau).\n","        cosines = torch.cos(\n","            taus.view(batch_size, N, 1) * i_pi\n","            ).view(batch_size * N, self.num_cosines)\n","\n","        # Calculate embeddings of taus.\n","        tau_embeddings = self.net(cosines).view(\n","            batch_size, N, self.embedding_dim)\n","\n","        return tau_embeddings\n","\n","\n","class QuantileNetwork(nn.Module):\n","\n","    def __init__(self, num_actions, embedding_dim=7*7*64, dueling_net=False,\n","                 noisy_net=False):\n","        super(QuantileNetwork, self).__init__()\n","        linear = NoisyLinear if noisy_net else nn.Linear\n","\n","        if not dueling_net:\n","            self.net = nn.Sequential(\n","                linear(embedding_dim, 512),\n","                nn.ReLU(),\n","                linear(512, num_actions),\n","            )\n","        else:\n","            self.advantage_net = nn.Sequential(\n","                linear(embedding_dim, 512),\n","                nn.ReLU(),\n","                linear(512, num_actions),\n","            )\n","            self.baseline_net = nn.Sequential(\n","                linear(embedding_dim, 512),\n","                nn.ReLU(),\n","                linear(512, 1),\n","            )\n","\n","        self.num_actions = num_actions\n","        self.embedding_dim = embedding_dim\n","        self.dueling_net = dueling_net\n","        self.noisy_net = noisy_net\n","\n","    def forward(self, state_embeddings, tau_embeddings):\n","        assert state_embeddings.shape[0] == tau_embeddings.shape[0]\n","        assert state_embeddings.shape[1] == tau_embeddings.shape[2]\n","\n","        # NOTE: Because variable taus correspond to either \\tau or \\hat \\tau\n","        # in the paper, N isn't neccesarily the same as fqf.N.\n","        batch_size = state_embeddings.shape[0]\n","        N = tau_embeddings.shape[1]\n","\n","        # Reshape into (batch_size, 1, embedding_dim).\n","        state_embeddings = state_embeddings.view(\n","            batch_size, 1, self.embedding_dim)\n","\n","        # Calculate embeddings of states and taus.\n","        embeddings = (state_embeddings * tau_embeddings).view(\n","            batch_size * N, self.embedding_dim)\n","\n","        # Calculate quantile values.\n","        if not self.dueling_net:\n","            quantiles = self.net(embeddings)\n","        else:\n","            advantages = self.advantage_net(embeddings)\n","            baselines = self.baseline_net(embeddings)\n","            quantiles =\\\n","                baselines + advantages - advantages.mean(1, keepdim=True)\n","\n","        return quantiles.view(batch_size, N, self.num_actions)\n","\n","\n","class NoisyLinear(nn.Module):\n","    def __init__(self, in_features, out_features, sigma=0.5):\n","        super(NoisyLinear, self).__init__()\n","\n","        # Learnable parameters.\n","        self.mu_W = nn.Parameter(\n","            torch.FloatTensor(out_features, in_features))\n","        self.sigma_W = nn.Parameter(\n","            torch.FloatTensor(out_features, in_features))\n","        self.mu_bias = nn.Parameter(torch.FloatTensor(out_features))\n","        self.sigma_bias = nn.Parameter(torch.FloatTensor(out_features))\n","\n","        # Factorized noise parameters.\n","        self.register_buffer('eps_p', torch.FloatTensor(in_features))\n","        self.register_buffer('eps_q', torch.FloatTensor(out_features))\n","\n","        self.in_features = in_features\n","        self.out_features = out_features\n","        self.sigma = sigma\n","\n","        self.reset()\n","        self.sample()\n","\n","    def reset(self):\n","        bound = 1 / np.sqrt(self.in_features)\n","        self.mu_W.data.uniform_(-bound, bound)\n","        self.mu_bias.data.uniform_(-bound, bound)\n","        self.sigma_W.data.fill_(self.sigma / np.sqrt(self.in_features))\n","        self.sigma_bias.data.fill_(self.sigma / np.sqrt(self.out_features))\n","\n","    def f(self, x):\n","        return x.normal_().sign().mul(x.abs().sqrt())\n","\n","    def sample(self):\n","        self.eps_p.copy_(self.f(self.eps_p))\n","        self.eps_q.copy_(self.f(self.eps_q))\n","\n","    def forward(self, x):\n","        if self.training:\n","            weight = self.mu_W + self.sigma_W * self.eps_q.ger(self.eps_p)\n","            bias = self.mu_bias + self.sigma_bias * self.eps_q.clone()\n","        else:\n","            weight = self.mu_W\n","            bias = self.mu_bias\n","\n","        return F.linear(x, weight, bias)"]},{"cell_type":"markdown","metadata":{"id":"pBqRPr_kpEXc"},"source":["### IQN Memory"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CfWef0CDo2fX"},"outputs":[],"source":["from collections import deque\n","import numpy as np\n","import torch\n","\n","\n","class MultiStepBuff:\n","\n","    def __init__(self, maxlen=3):\n","        super(MultiStepBuff, self).__init__()\n","        self.maxlen = int(maxlen)\n","        self.reset()\n","\n","    def append(self, state, action, reward):\n","        self.states.append(state)\n","        self.actions.append(action)\n","        self.rewards.append(reward)\n","\n","    def get(self, gamma=0.99):\n","        assert len(self.rewards) > 0\n","        state = self.states.popleft()\n","        action = self.actions.popleft()\n","        reward = self._nstep_return(gamma)\n","        return state, action, reward\n","\n","    def _nstep_return(self, gamma):\n","        r = np.sum([r * (gamma ** i) for i, r in enumerate(self.rewards)])\n","        self.rewards.popleft()\n","        return r\n","\n","    def reset(self):\n","        # Buffer to store n-step transitions.\n","        self.states = deque(maxlen=self.maxlen)\n","        self.actions = deque(maxlen=self.maxlen)\n","        self.rewards = deque(maxlen=self.maxlen)\n","\n","    def is_empty(self):\n","        return len(self.rewards) == 0\n","\n","    def is_full(self):\n","        return len(self.rewards) == self.maxlen\n","\n","    def __len__(self):\n","        return len(self.rewards)\n","\n","\n","class LazyMemory(dict):\n","    state_keys = ['state', 'next_state']\n","    np_keys = ['action', 'reward', 'done']\n","    keys = state_keys + np_keys\n","\n","    def __init__(self, capacity, state_shape, device):\n","        super(LazyMemory, self).__init__()\n","        self.capacity = int(capacity)\n","        self.state_shape = state_shape\n","        self.device = device\n","        self.reset()\n","\n","    def reset(self):\n","        self['state'] = []\n","        self['next_state'] = []\n","\n","        self['action'] = np.empty((self.capacity, 1), dtype=np.int64)\n","        self['reward'] = np.empty((self.capacity, 1), dtype=np.float32)\n","        self['done'] = np.empty((self.capacity, 1), dtype=np.float32)\n","\n","        self._n = 0\n","        self._p = 0\n","\n","    def append(self, state, action, reward, next_state, done,\n","               episode_done=None):\n","        self._append(state, action, reward, next_state, done)\n","\n","    def _append(self, state, action, reward, next_state, done):\n","        self['state'].append(state)\n","        self['next_state'].append(next_state)\n","        self['action'][self._p] = action\n","        self['reward'][self._p] = reward\n","        self['done'][self._p] = done\n","\n","        self._n = min(self._n + 1, self.capacity)\n","        self._p = (self._p + 1) % self.capacity\n","\n","        self.truncate()\n","\n","    def truncate(self):\n","        while len(self) > self.capacity:\n","            del self['state'][0]\n","            del self['next_state'][0]\n","\n","    def sample(self, batch_size):\n","        indices = np.random.randint(low=0, high=len(self), size=batch_size)\n","        return self._sample(indices, batch_size)\n","\n","    def _sample(self, indices, batch_size):\n","        bias = -self._p if self._n == self.capacity else 0\n","\n","        states = np.empty(\n","            (batch_size, *self.state_shape), dtype=np.uint8)\n","        next_states = np.empty(\n","            (batch_size, *self.state_shape), dtype=np.uint8)\n","\n","        for i, index in enumerate(indices):\n","            _index = np.mod(index+bias, self.capacity)\n","            states[i, ...] = self['state'][_index]\n","            next_states[i, ...] = self['next_state'][_index]\n","\n","        states = torch.ByteTensor(states).to(self.device).float() / 255.\n","        next_states = torch.ByteTensor(\n","            next_states).to(self.device).float() / 255.\n","        actions = torch.LongTensor(self['action'][indices]).to(self.device)\n","        rewards = torch.FloatTensor(self['reward'][indices]).to(self.device)\n","        dones = torch.FloatTensor(self['done'][indices]).to(self.device)\n","\n","        return states, actions, rewards, next_states, dones\n","\n","    def __len__(self):\n","        return len(self['state'])\n","\n","    def get(self):\n","        return dict(self)\n","\n","    def load(self, memory):\n","        for key in self.state_keys:\n","            self[key].extend(memory[key])\n","\n","        num_data = len(memory['state'])\n","        if self._p + num_data <= self.capacity:\n","            for key in self.np_keys:\n","                self[key][self._p:self._p+num_data] = memory[key]\n","        else:\n","            mid_index = self.capacity - self._p\n","            end_index = num_data - mid_index\n","            for key in self.np_keys:\n","                self[key][self._p:] = memory[key][:mid_index]\n","                self[key][:end_index] = memory[key][mid_index:]\n","\n","        self._n = min(self._n + num_data, self.capacity)\n","        self._p = (self._p + num_data) % self.capacity\n","        self.truncate()\n","        assert self._n == len(self)\n","\n","\n","class LazyMultiStepMemory(LazyMemory):\n","\n","    def __init__(self, capacity, state_shape, device, gamma=0.99,\n","                 multi_step=3):\n","        super(LazyMultiStepMemory, self).__init__(\n","            capacity, state_shape, device)\n","\n","        self.gamma = gamma\n","        self.multi_step = int(multi_step)\n","        if self.multi_step != 1:\n","            self.buff = MultiStepBuff(maxlen=self.multi_step)\n","\n","    def append(self, state, action, reward, next_state, done):\n","        if self.multi_step != 1:\n","            self.buff.append(state, action, reward)\n","\n","            if self.buff.is_full():\n","                state, action, reward = self.buff.get(self.gamma)\n","                self._append(state, action, reward, next_state, done)\n","\n","            if done:\n","                while not self.buff.is_empty():\n","                    state, action, reward = self.buff.get(self.gamma)\n","                    self._append(state, action, reward, next_state, done)\n","        else:\n","            self._append(state, action, reward, next_state, done)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a4G6ZqtmrRF-"},"outputs":[],"source":["import operator\n","\n","\n","class SegmentTree:\n","\n","    def __init__(self, size, op, init_val):\n","        assert size > 0 and size & (size - 1) == 0\n","        self._size = size\n","        self._op = op\n","        self._init_val = init_val\n","        self._values = [init_val for _ in range(2 * size)]\n","\n","    def _reduce(self, start=0, end=None):\n","        if end is None:\n","            end = self._size\n","        elif end < 0:\n","            end += self._size\n","\n","        start += self._size\n","        end += self._size\n","\n","        res = self._init_val\n","        while start < end:\n","            if start & 1:\n","                res = self._op(res, self._values[start])\n","                start += 1\n","\n","            if end & 1:\n","                end -= 1\n","                res = self._op(res, self._values[end])\n","\n","            start //= 2\n","            end //= 2\n","\n","        return res\n","\n","    def __setitem__(self, idx, val):\n","        assert 0 <= idx < self._size\n","\n","        # Set value.\n","        idx += self._size\n","        self._values[idx] = val\n","\n","        # Update its ancestors iteratively.\n","        idx = idx >> 1\n","        while idx >= 1:\n","            left = 2 * idx\n","            self._values[idx] = \\\n","                self._op(self._values[left], self._values[left + 1])\n","            idx = idx >> 1\n","\n","    def __getitem__(self, idx):\n","        assert 0 <= idx < self._size\n","        return self._values[idx + self._size]\n","\n","\n","class SumTree(SegmentTree):\n","\n","    def __init__(self, size):\n","        super().__init__(size, operator.add, 0.0)\n","\n","    def sum(self, start=0, end=None):\n","        return self._reduce(start, end)\n","\n","    def find_prefixsum_idx(self, prefixsum):\n","        assert 0 <= prefixsum <= self.sum() + 1e-5\n","        idx = 1\n","\n","        # Traverse to the leaf.\n","        while idx < self._size:\n","            left = 2 * idx\n","            if self._values[left] > prefixsum:\n","                idx = left\n","            else:\n","                prefixsum -= self._values[left]\n","                idx = left + 1\n","        return idx - self._size\n","\n","\n","class MinTree(SegmentTree):\n","\n","    def __init__(self, size):\n","        super().__init__(size, min, float(\"inf\"))\n","\n","    def min(self, start=0, end=None):\n","        return self._reduce(start, end)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5xbIyXhdrLoZ"},"outputs":[],"source":["import numpy as np\n","import torch\n","\n","\n","class LazyPrioritizedMultiStepMemory(LazyMultiStepMemory):\n","\n","    def __init__(self, capacity, state_shape, device, gamma=0.99,\n","                 multi_step=3, alpha=0.5, beta=0.4, beta_steps=2e5,\n","                 min_pa=0.0, max_pa=1.0, eps=0.01):\n","        super().__init__(capacity, state_shape, device, gamma, multi_step)\n","\n","        self.alpha = alpha\n","        self.beta = beta\n","        self.beta_diff = (1.0 - beta) / beta_steps\n","        self.min_pa = min_pa\n","        self.max_pa = max_pa\n","        self.eps = eps\n","        self._cached = None\n","\n","        it_capacity = 1\n","        while it_capacity < capacity:\n","            it_capacity *= 2\n","        self.it_sum = SumTree(it_capacity)\n","        self.it_min = MinTree(it_capacity)\n","\n","    def _pa(self, p):\n","        return np.clip((p + self.eps) ** self.alpha, self.min_pa, self.max_pa)\n","\n","    def append(self, state, action, reward, next_state, done, p=None):\n","        # Calculate priority.\n","        if p is None:\n","            pa = self.max_pa\n","        else:\n","            pa = self._pa(p)\n","\n","        if self.multi_step != 1:\n","            self.buff.append(state, action, reward)\n","\n","            if self.buff.is_full():\n","                state, action, reward = self.buff.get(self.gamma)\n","                self._append(state, action, reward, next_state, done, pa)\n","\n","            if done:\n","                while not self.buff.is_empty():\n","                    state, action, reward = self.buff.get(self.gamma)\n","                    self._append(state, action, reward, next_state, done, pa)\n","        else:\n","            self._append(state, action, reward, next_state, done, pa)\n","\n","    def _append(self, state, action, reward, next_state, done, pa):\n","        # Store priority, which is done efficiently by SegmentTree.\n","        self.it_min[self._p] = pa\n","        self.it_sum[self._p] = pa\n","        super()._append(state, action, reward, next_state, done)\n","\n","    def _sample_idxes(self, batch_size):\n","        total_pa = self.it_sum.sum(0, self._n)\n","        rands = np.random.rand(batch_size) * total_pa\n","        indices = [self.it_sum.find_prefixsum_idx(r) for r in rands]\n","        self.beta = min(1., self.beta + self.beta_diff)\n","        return indices\n","\n","    def sample(self, batch_size):\n","        assert self._cached is None, 'Update priorities before sampling.'\n","\n","        self._cached = self._sample_idxes(batch_size)\n","        batch = self._sample(self._cached, batch_size)\n","        weights = self._calc_weights(self._cached)\n","        return batch, weights\n","\n","    def _calc_weights(self, indices):\n","        min_pa = self.it_min.min()\n","        weights = [(self.it_sum[i] / min_pa) ** -self.beta for i in indices]\n","        return torch.FloatTensor(weights).to(self.device).view(-1, 1)\n","\n","    def update_priority(self, errors):\n","        assert self._cached is not None\n","\n","        ps = errors.detach().cpu().abs().numpy().flatten()\n","        pas = self._pa(ps)\n","\n","        for index, pa in zip(self._cached, pas):\n","            assert 0 <= index < self._n\n","            assert 0 < pa\n","            self.it_sum[index] = pa\n","            self.it_min[index] = pa\n","\n","        self._cached = None\n"]},{"cell_type":"markdown","metadata":{"id":"8e75j4QEptLC"},"source":["### IQN Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FBfnm0lApueF"},"outputs":[],"source":["class BaseModel(nn.Module):\n","\n","    def __init__(self):\n","        super().__init__()\n","\n","    def sample_noise(self):\n","        if self.noisy_net:\n","            for m in self.modules():\n","                if isinstance(m, NoisyLinear):\n","                    m.sample()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CxI68ILTpwaM"},"outputs":[],"source":["import torch\n","\n","class IQN(BaseModel):\n","\n","    def __init__(self, num_channels, num_actions, K=32, num_cosines=32,\n","                 embedding_dim=7*7*64, dueling_net=False, noisy_net=False):\n","        super(IQN, self).__init__()\n","\n","        # Feature extractor of DQN.\n","        self.dqn_net = DQNBase(num_channels=num_channels)\n","        # Cosine embedding network.\n","        self.cosine_net = CosineEmbeddingNetwork(\n","            num_cosines=num_cosines, embedding_dim=embedding_dim,\n","            noisy_net=noisy_net)\n","        # Quantile network.\n","        self.quantile_net = QuantileNetwork(\n","            num_actions=num_actions, dueling_net=dueling_net,\n","            noisy_net=noisy_net)\n","\n","        self.K = K\n","        self.num_channels = num_channels\n","        self.num_actions = num_actions\n","        self.num_cosines = num_cosines\n","        self.embedding_dim = embedding_dim\n","        self.dueling_net = dueling_net\n","        self.noisy_net = noisy_net\n","\n","    def calculate_state_embeddings(self, states):\n","        return self.dqn_net(states)\n","\n","    def calculate_quantiles(self, taus, states=None, state_embeddings=None):\n","        assert states is not None or state_embeddings is not None\n","\n","        if state_embeddings is None:\n","            state_embeddings = self.dqn_net(states)\n","\n","        tau_embeddings = self.cosine_net(taus)\n","        return self.quantile_net(state_embeddings, tau_embeddings)\n","\n","    def calculate_q(self, states=None, state_embeddings=None):\n","        assert states is not None or state_embeddings is not None\n","        batch_size = states.shape[0] if states is not None\\\n","            else state_embeddings.shape[0]\n","\n","        if state_embeddings is None:\n","            state_embeddings = self.dqn_net(states)\n","\n","        # Sample fractions.\n","        taus = torch.rand(\n","            batch_size, self.K, dtype=state_embeddings.dtype,\n","            device=state_embeddings.device)\n","\n","        # Calculate quantiles.\n","        quantiles = self.calculate_quantiles(\n","            taus, state_embeddings=state_embeddings)\n","        assert quantiles.shape == (batch_size, self.K, self.num_actions)\n","        #print(f\"quantiles shape: {quantiles.shape}\")\n","\n","        # Calculate expectations of value distributions.\n","        q = quantiles.mean(dim=1)\n","        assert q.shape == (batch_size, self.num_actions)\n","\n","        return q, quantiles\n"]},{"cell_type":"markdown","metadata":{"id":"y3210QiPpJEd"},"source":["### IQN"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YbRbPIyDowb6"},"outputs":[],"source":["from abc import ABC, abstractmethod\n","import os\n","import numpy as np\n","import torch\n","from torch.utils.tensorboard import SummaryWriter\n","\n","class BaseAgent(ABC):\n","\n","    def __init__(self, observation_space, action_space, num_steps=100000,\n","                 batch_size=32, memory_size=10**6, gamma=0.99, multi_step=1,\n","                 update_interval=4, target_update_interval=10000,\n","                 start_steps=50000, epsilon_train=0.01, epsilon_eval=0.001,\n","                 epsilon_decay_steps=250000, double_q_learning=False,\n","                 dueling_net=False, noisy_net=False, use_per=False,\n","                 log_interval=100, eval_interval=250000, num_eval_steps=125000,\n","                 max_episode_steps=27000, grad_cliping=5.0, cuda=True, seed=0):\n","\n","        # torch.backends.cudnn.deterministic = True  # It harms a performance.\n","        # torch.backends.cudnn.benchmark = False  # It harms a performance.\n","\n","        self.device = \"cuda\" if cuda and torch.cuda.is_available() else \"cpu\"\n","\n","        self.online_net = None\n","        self.target_net = None\n","\n","        # Replay memory which is memory-efficient to store stacked frames.\n","        if use_per:\n","            beta_steps = (num_steps - start_steps) / update_interval\n","            self.memory = LazyPrioritizedMultiStepMemory(\n","                memory_size, observation_space.shape,\n","                self.device, gamma, multi_step, beta_steps=beta_steps)\n","        else:\n","            self.memory = LazyMultiStepMemory(\n","                memory_size, observation_space.shape,\n","                self.device, gamma, multi_step)\n","\n","        self.steps = 0\n","        self.learning_steps = 0\n","        self.episodes = 0\n","        self.best_eval_score = -np.inf\n","        self.num_actions = action_space.n\n","        self.num_steps = num_steps\n","        self.batch_size = batch_size\n","\n","        self.double_q_learning = double_q_learning\n","        self.dueling_net = dueling_net\n","        self.noisy_net = noisy_net\n","        self.use_per = use_per\n","\n","        self.log_interval = log_interval\n","        self.eval_interval = eval_interval\n","        self.num_eval_steps = num_eval_steps\n","        self.gamma_n = gamma ** multi_step\n","        self.start_steps = start_steps\n","        self.epsilon_train = LinearAnneaer(\n","            1.0, epsilon_train, epsilon_decay_steps)\n","        self.epsilon_eval = epsilon_eval\n","        self.update_interval = update_interval\n","        self.target_update_interval = target_update_interval\n","        self.max_episode_steps = max_episode_steps\n","        self.grad_cliping = grad_cliping\n","\n","        print(f\"Agent Initialized: double q learning {self.double_q_learning}\")\n","        print(f\"Agent Initialized: dueling net {self.dueling_net}\")\n","        print(f\"Agent Initialized: noisy net {self.noisy_net}\")\n","        print(f\"Agent Initialized: use per {self.use_per}\")\n","\n","    def is_update(self):\n","        return self.steps % self.update_interval == 0\\\n","            and self.steps >= self.start_steps\n","\n","    def is_random(self, eval=False):\n","        # Use e-greedy for evaluation.\n","        if self.steps < self.start_steps:\n","            return True\n","        if eval:\n","            return np.random.rand() < self.epsilon_eval\n","        if self.noisy_net:\n","            return False\n","        return np.random.rand() < self.epsilon_train.get()\n","\n","    def update_target(self):\n","        self.target_net.load_state_dict(\n","            self.online_net.state_dict())\n","\n","    def explore(self):\n","        # Act with randomness.\n","        action = self.env.action_space.sample()\n","        return action\n","\n","    def exploit(self, state):\n","        # Act without randomness.\n","        state = torch.from_numpy(np.array(state)).unsqueeze(0).to(self.device)\n","        with torch.no_grad():\n","            action = self.online_net.calculate_q(states=state).argmax().item()\n","        return action\n","\n","    def train_episode(self):\n","        self.online_net.train()\n","        self.target_net.train()\n","\n","        self.episodes += 1\n","        episode_return = 0.\n","        episode_steps = 0\n","\n","        done = False\n","        state, _ = self.env.reset()\n","\n","        while (not done) and episode_steps <= self.max_episode_steps:\n","            # NOTE: Noises can be sampled only after self.learn(). However, I\n","            # sample noises before every action, which seems to lead better\n","            # performances.\n","            self.online_net.sample_noise()\n","\n","            if self.is_random(eval=False):\n","                action = self.explore()\n","            else:\n","                action = self.exploit(state)\n","            done = False\n","            next_state, reward, trunc, term, _ = self.env.step(action)\n","            if term or trunc:\n","              done = True\n","\n","            # To calculate efficiently, I just set priority=max_priority here.\n","            self.memory.append(state, action, reward, next_state, done)\n","\n","            self.steps += 1\n","            episode_steps += 1\n","            episode_return += reward\n","            state = next_state\n","\n","            self.train_step_interval()\n","\n","        # We log running mean of stats.\n","        self.train_return.append(episode_return)\n","\n","        # We log evaluation results along with training frames = 4 * steps.\n","        if self.episodes % self.log_interval == 0:\n","            self.writer.add_scalar(\n","                'return/train', self.train_return.get(), 4 * self.steps)\n","\n","        print(f'Episode: {self.episodes:<4}  '\n","              f'episode steps: {episode_steps:<4}  '\n","              f'return: {episode_return:<5.1f}')\n","\n","    def train_step_interval(self):\n","        self.epsilon_train.step()\n","\n","        if self.steps % self.target_update_interval == 0:\n","            self.update_target()\n","\n","        if self.is_update():\n","            self.learn()\n","\n","        if self.steps % self.eval_interval == 0:\n","            self.evaluate()\n","            self.save_models(os.path.join(self.model_dir, 'final'))\n","            self.online_net.train()\n","\n","    def evaluate(self):\n","        self.online_net.eval()\n","        num_episodes = 0\n","        num_steps = 0\n","        total_return = 0.0\n","\n","        while True:\n","            state = self.test_env.reset()\n","            episode_steps = 0\n","            episode_return = 0.0\n","            done = False\n","            while (not done) and episode_steps <= self.max_episode_steps:\n","                if self.is_random(eval=True):\n","                    action = self.explore()\n","                else:\n","                    action = self.exploit(state)\n","\n","                next_state, reward, done, _ = self.test_env.step(action)\n","                num_steps += 1\n","                episode_steps += 1\n","                episode_return += reward\n","                state = next_state\n","\n","            num_episodes += 1\n","            total_return += episode_return\n","\n","            if num_steps > self.num_eval_steps:\n","                break\n","\n","        mean_return = total_return / num_episodes\n","\n","        if mean_return > self.best_eval_score:\n","            self.best_eval_score = mean_return\n","            self.save_models(os.path.join(self.model_dir, 'best'))\n","\n","        # We log evaluation results along with training frames = 4 * steps.\n","        self.writer.add_scalar(\n","            'return/test', mean_return, 4 * self.steps)\n","        print('-' * 60)\n","        print(f'Num steps: {self.steps:<5}  '\n","              f'return: {mean_return:<5.1f}')\n","        print('-' * 60)\n","\n","    def __del__(self):\n","        self.env.close()\n","        self.test_env.close()\n","        self.writer.close()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E8PhDIDKpjoK"},"outputs":[],"source":["import torch\n","from torch.optim import Adam\n","\n","class IQNAgent(BaseAgent):\n","\n","    def __init__(self, observation_space, action_space, N, N_dash,\n","                 double_q_learning, dueling_net, noisy_net, use_per,\n","                 num_steps=5*(10**7), batch_size=32, K=32, num_cosines=64,\n","                 kappa=1.0, lr=5e-5, memory_size=10**6, gamma=0.99,\n","                 multi_step=1, update_interval=4, target_update_interval=10000,\n","                 start_steps=50000, epsilon_train=0.01, epsilon_eval=0.001,\n","                 epsilon_decay_steps=250000,\n","                 log_interval=100, eval_interval=250000, num_eval_steps=125000,\n","                 max_episode_steps=27000, grad_cliping=None, cuda=True,\n","                 seed=0):\n","        super(IQNAgent, self).__init__(\n","            observation_space, action_space, num_steps, batch_size, memory_size,\n","            gamma, multi_step, update_interval, target_update_interval,\n","            start_steps, epsilon_train, epsilon_eval, epsilon_decay_steps,\n","            double_q_learning, dueling_net, noisy_net, use_per, log_interval,\n","            eval_interval, num_eval_steps, max_episode_steps, grad_cliping,\n","            cuda, seed)\n","\n","        # Online network.\n","        self.online_net = IQN(\n","            num_channels=observation_space.shape[0],\n","            num_actions=self.num_actions, K=K, num_cosines=num_cosines,\n","            dueling_net=dueling_net, noisy_net=noisy_net).to(self.device)\n","        # Target network.\n","        self.target_net = IQN(\n","            num_channels=observation_space.shape[0],\n","            num_actions=self.num_actions, K=K, num_cosines=num_cosines,\n","            dueling_net=dueling_net, noisy_net=noisy_net).to(self.device)\n","\n","        # Copy parameters of the learning network to the target network.\n","        self.update_target()\n","        # Disable calculations of gradients of the target network.\n","        disable_gradients(self.target_net)\n","\n","        self.optimizer = Adam(\n","            self.online_net.parameters(),\n","            lr=lr, eps=1e-2/batch_size)\n","\n","        self.N = N\n","        self.N_dash = N_dash\n","        self.K = K\n","        self.num_cosines = num_cosines\n","        self.kappa = kappa\n","\n","    def train(self, states, actions, rewards, next_states, dones, weights=None):\n","        self.learning_steps += 1\n","        self.online_net.sample_noise()\n","        self.target_net.sample_noise()\n","\n","        \"\"\"if self.use_per:\n","            (states, actions, rewards, next_states, dones), weights =\\\n","                self.memory.sample(self.batch_size)\n","        else:\n","            states, actions, rewards, next_states, dones =\\\n","                self.memory.sample(self.batch_size)\n","            weights = None\"\"\"\n","\n","        # Calculate features of states.\n","        state_embeddings = self.online_net.calculate_state_embeddings(states)\n","\n","        quantile_loss, mean_q, errors = self.calculate_loss(\n","            state_embeddings, actions, rewards, next_states, dones, weights)\n","        #self.writer.add_scalar(\"loss/quantile_loss_every_step\", quantile_loss.detach().item(), self.steps)\n","        assert errors.shape == (self.batch_size, 1)\n","\n","        update_params(\n","            self.optimizer, quantile_loss,\n","            networks=[self.online_net],\n","            retain_graph=False, grad_cliping=self.grad_cliping)\n","\n","\n","        \"\"\"if 4*self.steps % self.log_interval == 0:\n","            self.writer.add_scalar(\n","                'loss/quantile_loss', quantile_loss.detach().item(),\n","                4*self.steps)\n","            self.writer.add_scalar('stats/mean_Q', mean_q, 4*self.steps)\"\"\"\n","        return quantile_loss.detach().item(), errors\n","\n","    def calculate_loss(self, state_embeddings, actions, rewards, next_states,\n","                       dones, weights):\n","        # Sample fractions.\n","        taus = torch.rand(\n","            self.batch_size, self.N, dtype=state_embeddings.dtype,\n","            device=state_embeddings.device)\n","\n","        # Calculate quantile values of current states and actions at tau_hats.\n","        current_sa_quantiles = evaluate_quantile_at_action(\n","            self.online_net.calculate_quantiles(\n","                taus, state_embeddings=state_embeddings),\n","            actions)\n","        assert current_sa_quantiles.shape == (\n","            self.batch_size, self.N, 1)\n","\n","        with torch.no_grad():\n","            # Calculate Q values of next states.\n","            if self.double_q_learning:\n","                # Sample the noise of online network to decorrelate between\n","                # the action selection and the quantile calculation.\n","                self.online_net.sample_noise()\n","                next_q, _ = self.online_net.calculate_q(states=next_states)\n","            else:\n","                next_state_embeddings =\\\n","                    self.target_net.calculate_state_embeddings(next_states)\n","                next_q, _ = self.target_net.calculate_q(\n","                    state_embeddings=next_state_embeddings)\n","\n","            # Calculate greedy actions.\n","            next_actions = torch.argmax(next_q, dim=1, keepdim=True)\n","            assert next_actions.shape == (self.batch_size, 1)\n","\n","            # Calculate features of next states.\n","            if self.double_q_learning:\n","                next_state_embeddings =\\\n","                    self.target_net.calculate_state_embeddings(next_states)\n","\n","            # Sample next fractions.\n","            tau_dashes = torch.rand(\n","                self.batch_size, self.N_dash, dtype=state_embeddings.dtype,\n","                device=state_embeddings.device)\n","\n","            # Calculate quantile values of next states and next actions.\n","            next_sa_quantiles = evaluate_quantile_at_action(\n","                self.target_net.calculate_quantiles(\n","                    tau_dashes, state_embeddings=next_state_embeddings\n","                ), next_actions).transpose(1, 2)\n","            assert next_sa_quantiles.shape == (self.batch_size, 1, self.N_dash)\n","\n","            # Calculate target quantile values.\n","            target_sa_quantiles = rewards[..., None] + (\n","                1.0 - dones[..., None]) * self.gamma_n * next_sa_quantiles\n","            assert target_sa_quantiles.shape == (\n","                self.batch_size, 1, self.N_dash)\n","\n","        td_errors = target_sa_quantiles - current_sa_quantiles\n","        assert td_errors.shape == (self.batch_size, self.N, self.N_dash)\n","\n","        quantile_huber_loss = calculate_quantile_huber_loss(\n","            td_errors, taus, weights, self.kappa)\n","\n","        return quantile_huber_loss, next_q.detach().mean().item(), \\\n","            td_errors.detach().abs().sum(dim=1).mean(dim=1, keepdim=True)\n"]},{"cell_type":"markdown","metadata":{"id":"sOTWS-NHmdQY"},"source":["## PPO Train"]},{"cell_type":"markdown","metadata":{"id":"M14TrXHBqIMs"},"source":["#### PPO Class"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nNyzlCPPU5TO"},"outputs":[],"source":["class PPO():\n","  def __init__(self, args):\n","    self.args = args\n","    self.exp_name = args.exp_name\n","    self.seed = args.seed\n","    self.torch_deterministic = args.torch_deterministic\n","    self.cuda = args.cuda\n","    self.track = args.track\n","    self.capture_video = args.capture_video\n","\n","    # Algorithm specific arguments\n","    self.env_id = args.env_id\n","    self.total_timesteps = args.total_timesteps\n","    self.learning_rate: float = 2.5e-4\n","    self.num_envs = args.num_envs\n","    self.num_steps = args.num_steps\n","    self.anneal_lr = args.anneal_lr\n","    self.gamma = args.gamma\n","    self.gae_lambda = args.gae_lambda\n","    self.num_minibatches = args.num_minibatches\n","    self.update_epochs = args.update_epochs\n","    self.norm_adv = args.norm_adv\n","    self.clip_coef = args.clip_coef\n","    self.clip_vloss = args.clip_vloss\n","    self.ent_coef = args.ent_coef\n","    self.vf_coef = args.vf_coef\n","    self.max_grad_norm = args.max_grad_norm\n","    self.target_kl = args.target_kl\n","    self.batch_size = int(args.num_envs * args.num_steps)\n","    self.minibatch_size = int(self.batch_size // args.num_minibatches)\n","    self.num_iterations = args.total_timesteps // self.batch_size\n","\n","    self.run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}__{int(time.time())}\"\n","    writer = SummaryWriter(f\"/content/drive/MyDrive/code/PGRainbow/SpaceInvaders/cleanrl/runs/test{run_name}\")\n","    writer.add_text(\n","        \"hyperparameters\",\n","        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n","    )\n","\n","    random.seed(args.seed)\n","    np.random.seed(args.seed)\n","    torch.manual_seed(args.seed)\n","    torch.backends.cudnn.deterministic = args.torch_deterministic\n","\n","    self.device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n","    self.envs = envpool.make(\n","        args.env_id,\n","        env_type=\"gym\",\n","        num_envs=args.num_envs,\n","        episodic_life=True,\n","        reward_clip=True,\n","        seed=args.seed,\n","    )\n","\n","    self.envs.num_envs = args.num_envs\n","    self.envs.single_action_space = self.envs.action_space\n","    self.envs.single_observation_space = self.envs.observation_space\n","    self.envs = RecordEpisodeStatistics(self.envs)\n","    assert isinstance(self.envs.action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n","\n","    self.agent = Agent(self.envs).to(self.device)\n","    self.optimizer = optim.Adam(self.agent.parameters(), lr=args.learning_rate, eps=1e-5)\n","\n","    ## Storage\n","    self.obs = torch.zeros((args.num_steps, args.num_envs) + self.envs.single_observation_space.shape).to(self.device)\n","    self.actions = torch.zeros((args.num_steps, args.num_envs) + self.envs.single_action_space.shape).to(self.device)\n","    self.logprobs = torch.zeros((args.num_steps, args.num_envs)).to(self.device)\n","    self.rewards = torch.zeros((args.num_steps, args.num_envs)).to(self.device)\n","    self.dones = torch.zeros((args.num_steps, args.num_envs)).to(self.device)\n","    self.values = torch.zeros((args.num_steps, args.num_envs)).to(self.device)\n","    self.avg_returns = deque(maxlen=20)\n","\n","  def train(self, save_model=False):\n","    global_step = 0\n","    start_time = time.time()\n","    next_obs = torch.Tensor(envs.reset()).to(self.device)\n","    next_done = torch.zeros(args.num_envs).to(self.device)\n","\n","    loop_range = tqdm.tqdm(range(0, self.num_iterations + 1))\n","    for iteration in loop_range:\n","      if self.anneal_lr:\n","        frac = 1.0 - (iteration - 1.0) / self.num_iterations\n","        lrnow = frac * self.learning_rate\n","        self.optimizer.param_groups[0][\"lr\"] = lrnow\n","\n","      for step in range(0, self.num_steps):\n","        global_step += self.num_envs\n","        self.obs[step] = next_obs\n","        self.dones[step] = next_done\n","\n","        with torch.no_grad():\n","          action, logprob, _, value = self.agent.get_action_and_value(next_obs)\n","          self.values[step] = value.flatten()\n","        self.actions[step] = action\n","        self.logprobs[step] = logprob\n","\n","        next_obs, reward, next_done, info = self.envs.step(action.cpu().numpy())\n","        self.rewards[step] = torch.tensor(reward).to(self.device).view(-1)\n","        next_obs, next_done = torch.Tensor(next_obs).to(self.device), torch.Tensor(next_done).to(self.device)\n","\n","        for idx, d in enumerate(next_done):\n","          if d and info[\"lives\"][idx] == 0:\n","              print(f\"global_step={global_step}, episodic_return={info['r'][idx]}\")\n","              avg_returns.append(info[\"r\"][idx])\n","              writer.add_scalar(\"charts/avg_episodic_return\", np.average(avg_returns), global_step)\n","              writer.add_scalar(\"charts/episodic_return\", info[\"r\"][idx], global_step)\n","              writer.add_scalar(\"charts/episodic_length\", info[\"l\"][idx], global_step)\n","\n","      with torch.no_grad():\n","        next_value = self.agent.get_value(next_obs).reshape(1, -1)\n","        advantages = torch.zeros_like(rewards).to(device)\n","        lastgaelam = 0\n","        for t in reversed(range(self.num_steps)):\n","          if t == self.num_steps - 1:\n","            nextnonterminal = 1.0 - next_done\n","            nextvalues = next_value\n","          else:\n","            nextnonterminal = 1.0 - self.dones[t + 1]\n","            nextvalues = self.values[t + 1]\n","          delta = self.rewards[t] + self.gamma * nextvalues * nextnonterminal - self.values[t]\n","          advantages[t] = lastgaelam = delta + self.gamma * self.gae_lambda * nextnonterminal * lastgaelam\n","        returns = advantages + values\n","\n","      self.optimize()\n","\n","    envs.close()\n","    writer.close()\n","\n","    if save_model is True:\n","      self.save()\n","\n","  def optimize(self):\n","      b_obs = self.obs.reshape(-1, self.envs.single_observation_space.shape)\n","      b_logprobs = self.logprobs.reshape(-1)\n","      b_actions = self.actions.reshape((-1,) + self.envs.single_action_space.shape)\n","      b_advantages = advantages.reshape(-1)\n","      b_values = self.values.reshape(-1)\n","\n","      b_inds = np.arange(self.batch_size)\n","      clipfracs = []\n","      for _ in range(self.update_epochs):\n","        np.random.shuffle(b_inds)\n","        for start in range(0, self.batch_size, self.minibatch_size):\n","          end = start + self.minibatch_size\n","          mb_inds = b_inds[start:end]\n","\n","          _, newlogprob, entropy, newvalue = self.agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n","          logratio = newlogprob - b_logprobs[mb_inds]\n","          ratio = logratio.exp()\n","\n","          with torch.no_grad():\n","            old_approx_kl = (-logratio).mean()\n","            approx_kl = ((ratio - 1) - logratio).mean()\n","            clipfracs += [((ratio - 1.0).abs() > self.clip_coef).float().mean().item()]\n","\n","          mb_advantages = b_advantages[mb_inds]\n","          if self.norm_adv:\n","            mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n","\n","          # Policy Loss\n","          pg_loss1 = -mb_advantages * ratio\n","          pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - self.clip_coef, 1 + self.clip_coef)\n","          pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n","\n","          # Value Loss\n","          newvalue = newvalue.view(-1)\n","          v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n","          v_clipped = b_values[mb_inds] + torch.clamp(\n","              newvalue - b_values[mb_inds],\n","              -self.clip_coef,\n","              self.clip_coef\n","          )\n","          v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n","          v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n","          v_loss = 0.5 * v_loss_max.mean()\n","\n","          entropy_loss = entropy.mean()\n","          loss = pg_loss - self.ent_coef * entropy_loss + v_loss * self.vf_coef\n","\n","          optimizer.zero_grad()\n","          loss.backward()\n","          nn.utils.clip_grad_norm_(self.agent.parameters(), self.max_grad_norm)\n","          optimizer.step()\n","\n","        if self.target_kl is not None and approx_kl > self.target_kl:\n","          break\n","\n","      y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n","      var_y = np.var(y_true)\n","      explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n","\n","      self.writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n","      self.writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n","      self.writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n","      self.writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n","      self.writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n","      self.writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n","      self.writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n","      self.writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n","      print(\"SPS:\", int(global_step / (time.time() - start_time)))\n","      self.writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n","\n","  def save(self):\n","    save_path = f\"/content/drive/MyDrive/code/PGRainbow/SpaceInvaders/cleanrl/runs/{self.run_name}\" + \"model.pth\"\n","    print(f\"Saving Model to {save_path}\")\n","    torch.save({\n","            'model_state_dict': self.agent.state_dict(),\n","            'optimizer_state_dict': self.optimizer.state_dict(),\n","            }, save_path)\n","\n","  def load_model(self, path):\n","    checkpoint = torch.load(path)\n","    self.agent.load_state_dict(checkpoint['model_state_dict'])\n","    self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"]},{"cell_type":"markdown","metadata":{"id":"4DbonYRbqK7v"},"source":["#### CleanRL Copy Paste"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eMg-D6VLqZdA"},"outputs":[],"source":["@dataclass\n","class PPOArgs:\n","    exp_name: str = \"ppo_run1\"\n","    \"\"\"the name of this experiment\"\"\"\n","    seed: int = 1\n","    \"\"\"seed of the experiment\"\"\"\n","    torch_deterministic: bool = True\n","    \"\"\"if toggled, `torch.backends.cudnn.deterministic=False`\"\"\"\n","    cuda: bool = True\n","    \"\"\"if toggled, cuda will be enabled by default\"\"\"\n","    track: bool = False\n","    \"\"\"if toggled, this experiment will be tracked with Weights and Biases\"\"\"\n","    wandb_project_name: str = \"cleanRL\"\n","    \"\"\"the wandb's project name\"\"\"\n","    wandb_entity: str = None\n","    \"\"\"the entity (team) of wandb's project\"\"\"\n","    capture_video: bool = False\n","    \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n","\n","    # Algorithm specific arguments\n","    env_id: str = \"Crossbow-v5\"\n","    \"\"\"the id of the environment\"\"\"\n","    total_timesteps: int = 1000000\n","    \"\"\"total timesteps of the experiments\"\"\"\n","    learning_rate: float = 2.5e-4\n","    \"\"\"the learning rate of the optimizer\"\"\"\n","    num_envs: int = 8\n","    \"\"\"the number of parallel game environments\"\"\"\n","    num_steps: int = 128\n","    \"\"\"the number of steps to run in each environment per policy rollout\"\"\"\n","    anneal_lr: bool = True\n","    \"\"\"Toggle learning rate annealing for policy and value networks\"\"\"\n","    gamma: float = 0.99\n","    \"\"\"the discount factor gamma\"\"\"\n","    gae_lambda: float = 0.95\n","    \"\"\"the lambda for the general advantage estimation\"\"\"\n","    num_minibatches: int = 4\n","    \"\"\"the number of mini-batches\"\"\"\n","    update_epochs: int = 4\n","    \"\"\"the K epochs to update the policy\"\"\"\n","    norm_adv: bool = True\n","    \"\"\"Toggles advantages normalization\"\"\"\n","    clip_coef: float = 0.1\n","    \"\"\"the surrogate clipping coefficient\"\"\"\n","    clip_vloss: bool = True\n","    \"\"\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\"\"\"\n","    ent_coef: float = 0.01\n","    \"\"\"coefficient of the entropy\"\"\"\n","    vf_coef: float = 0.5\n","    \"\"\"coefficient of the value function\"\"\"\n","    max_grad_norm: float = 0.5\n","    \"\"\"the maximum norm for the gradient clipping\"\"\"\n","    target_kl: float = None\n","    \"\"\"the target KL divergence threshold\"\"\"\n","\n","    batch_size: int = 8 * 128\n","    \"\"\"the batch size (computed in runtime)\"\"\"\n","    minibatch_size: int = 128 // 4\n","    \"\"\"the mini-batch size (computed in runtime)\"\"\"\n","    num_iterations: int = (1000000 // (8 * 128))\n","    \"\"\"the number of iterations (computed in runtime)\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2CzL-ABIYZNX"},"outputs":[],"source":["args = PPOArgs\n","run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}\"\n","\n","writer = SummaryWriter(f\"/content/drive/MyDrive/code/PGRainbow/Crossbow/{run_name}\")\n","writer.add_text(\n","    \"hyperparameters\",\n","    \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",")\n","\n","# TRY NOT TO MODIFY: seeding\n","random.seed(args.seed)\n","np.random.seed(args.seed)\n","torch.manual_seed(args.seed)\n","torch.backends.cudnn.deterministic = args.torch_deterministic\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n","\n","# env setup\n","envs = envpool.make(\n","    args.env_id,\n","    env_type=\"gym\",\n","    num_envs=args.num_envs,\n","    episodic_life=True,\n","    reward_clip=True,\n","    seed=args.seed,\n",")\n","envs.num_envs = args.num_envs\n","envs.single_action_space = envs.action_space\n","envs.single_observation_space = envs.observation_space\n","envs = RecordEpisodeStatistics(envs)\n","assert isinstance(envs.action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n","\n","agent = Agent(envs).to(device)\n","optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n","\n","# ALGO Logic: Storage setup\n","obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n","actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n","logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n","rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n","dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n","values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n","avg_returns = deque(maxlen=20)\n","# TRY NOT TO MODIFY: start the game\n","global_step = 0\n","start_time = time.time()\n","next_obs = torch.Tensor(envs.reset()).to(device)\n","next_done = torch.zeros(args.num_envs).to(device)\n","\n","loop_range = tqdm.tqdm(range(0, args.num_iterations + 1))\n","for iteration in loop_range:\n","    # Annealing the rate if instructed to do so.\n","    if args.anneal_lr:\n","        frac = 1.0 - (iteration - 1.0) / args.num_iterations\n","        lrnow = frac * args.learning_rate\n","        optimizer.param_groups[0][\"lr\"] = lrnow\n","\n","    for step in range(0, args.num_steps):\n","        global_step += args.num_envs\n","        obs[step] = next_obs\n","        dones[step] = next_done\n","\n","        # ALGO LOGIC: action logic\n","        with torch.no_grad():\n","            action, logprob, _, value = agent.get_action_and_value(next_obs)\n","            values[step] = value.flatten()\n","        actions[step] = action\n","        logprobs[step] = logprob\n","\n","        # TRY NOT TO MODIFY: execute the game and log data.\n","        next_obs, reward, next_done, info = envs.step(action.cpu().numpy())\n","        rewards[step] = torch.tensor(reward).to(device).view(-1)\n","        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)\n","\n","        for idx, d in enumerate(next_done):\n","            if d and info[\"lives\"][idx] == 0:\n","                print(f\"global_step={global_step}, episodic_return={info['r'][idx]}\")\n","                avg_returns.append(info[\"r\"][idx])\n","                writer.add_scalar(\"charts/avg_episodic_return\", np.average(avg_returns), global_step)\n","                writer.add_scalar(\"charts/episodic_return\", info[\"r\"][idx], global_step)\n","                writer.add_scalar(\"charts/episodic_length\", info[\"l\"][idx], global_step)\n","\n","    # bootstrap value if not done\n","    with torch.no_grad():\n","        next_value = agent.get_value(next_obs).reshape(1, -1)\n","        advantages = torch.zeros_like(rewards).to(device)\n","        lastgaelam = 0\n","        for t in reversed(range(args.num_steps)):\n","            if t == args.num_steps - 1:\n","                nextnonterminal = 1.0 - next_done\n","                nextvalues = next_value\n","            else:\n","                nextnonterminal = 1.0 - dones[t + 1]\n","                nextvalues = values[t + 1]\n","            delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n","            advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n","        returns = advantages + values\n","\n","    # flatten the batch\n","    b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n","    b_logprobs = logprobs.reshape(-1)\n","    b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n","    b_advantages = advantages.reshape(-1)\n","    b_returns = returns.reshape(-1)\n","    b_values = values.reshape(-1)\n","\n","    # Optimizing the policy and value network\n","    b_inds = np.arange(args.batch_size)\n","    clipfracs = []\n","    for epoch in range(args.update_epochs):\n","        np.random.shuffle(b_inds)\n","        for start in range(0, args.batch_size, args.minibatch_size):\n","            end = start + args.minibatch_size\n","            mb_inds = b_inds[start:end]\n","\n","            _, newlogprob, entropy, newvalue = agent.get_action_and_value(b_obs[mb_inds], b_actions.long()[mb_inds])\n","            logratio = newlogprob - b_logprobs[mb_inds]\n","            ratio = logratio.exp()\n","\n","            with torch.no_grad():\n","                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n","                old_approx_kl = (-logratio).mean()\n","                approx_kl = ((ratio - 1) - logratio).mean()\n","                clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n","\n","            mb_advantages = b_advantages[mb_inds]\n","            if args.norm_adv:\n","                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n","\n","            # Policy loss\n","            pg_loss1 = -mb_advantages * ratio\n","            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n","            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n","\n","            # Value loss\n","            newvalue = newvalue.view(-1)\n","            if args.clip_vloss:\n","                v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n","                v_clipped = b_values[mb_inds] + torch.clamp(\n","                    newvalue - b_values[mb_inds],\n","                    -args.clip_coef,\n","                    args.clip_coef,\n","                )\n","                v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n","                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n","                v_loss = 0.5 * v_loss_max.mean()\n","            else:\n","                v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n","\n","            entropy_loss = entropy.mean()\n","            loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n","            optimizer.step()\n","\n","        if args.target_kl is not None and approx_kl > args.target_kl:\n","            break\n","\n","    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n","    var_y = np.var(y_true)\n","    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n","\n","    # TRY NOT TO MODIFY: record rewards for plotting purposes\n","    writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n","    writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n","    writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n","    writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n","    writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n","    writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n","    writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n","    writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n","    print(\"SPS:\", int(global_step / (time.time() - start_time)))\n","    writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n","\n","envs.close()\n","writer.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uHcxaa4b84jt"},"outputs":[],"source":["# Save model\n","def save(agent, optimizer):\n","  save_path = f\"/content/drive/MyDrive/code/PGRainbow/DemonAttack/{run_name}\" + \"_model.pth\"\n","  print(f\"Saving Model to {save_path}\")\n","  torch.save({\n","          'model_state_dict': agent.state_dict(),\n","          'optimizer_state_dict': optimizer.state_dict(),\n","          }, save_path)\n","save(agent, optimizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_bi_WxPckxho"},"outputs":[],"source":["path = \"/content/drive/MyDrive/code/PGRainbow/DemonAttack/DemonAttack-v5__ppo_run2__1_model.pth\"\n","def load(agent, optimizer, path):\n","  checkpoint = torch.load(path)\n","  agent.load_state_dict(checkpoint['model_state_dict'])\n","  optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n","  print(f\"Model loaded from {path}\")\n","load(agent, optimizer, path)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"b4uCLxucKueF"},"outputs":[],"source":["args = PPOArgs\n","# TRY NOT TO MODIFY: seeding\n","random.seed(args.seed)\n","np.random.seed(args.seed)\n","torch.manual_seed(args.seed)\n","torch.backends.cudnn.deterministic = args.torch_deterministic\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n","\n","env_name = \"DemonAttack-v5\"\n","envs = envpool.make(\n","    env_name,\n","    env_type=\"gym\",\n","    num_envs=args.num_envs,\n","    episodic_life=True,\n","    reward_clip=True,\n","    seed=args.seed,\n",")\n","envs.num_envs = args.num_envs\n","envs.single_action_space = envs.action_space\n","envs.single_observation_space = envs.observation_space\n","envs = RecordEpisodeStatistics(envs)\n","\n","agent = Agent(envs).to(device)\n","optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n","\n","args.num_steps = 1000\n","\n","obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n","actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n","logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n","rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n","dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n","values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n","avg_returns = deque(maxlen=20)\n","# TRY NOT TO MODIFY: start the game\n","global_step = 0\n","start_time = time.time()\n","next_obs = torch.Tensor(envs.reset()).to(device)\n","next_done = torch.zeros(args.num_envs).to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"akxYpBvQlPZv"},"outputs":[],"source":["\"\"\"V(s_0)\"\"\"\n","trajectories = []\n","while len(trajectories) != 5000:\n","  for step in range(0, args.num_steps):\n","    global_step += args.num_envs\n","    obs[step] = next_obs\n","    dones[step] = next_done\n","\n","    # ALGO LOGIC: action logic\n","    with torch.no_grad():\n","        action, logprob, _, value = agent.get_action_and_value(next_obs)\n","        values[step] = value.flatten()\n","    actions[step] = action\n","    logprobs[step] = logprob\n","\n","    # TRY NOT TO MODIFY: execute the game and log data.\n","    next_obs, reward, next_done, info = envs.step(action.cpu().numpy())\n","    rewards[step] = torch.tensor(reward).to(device).view(-1)\n","    next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)\n","\n","    for idx, d in enumerate(next_done):\n","        if d and info[\"lives\"][idx] == 0:\n","            print(f\"global_step={global_step}, episodic_return={info['r'][idx]}\")\n","            avg_returns.append(info[\"r\"][idx])\n","            trajectories.append(info[\"r\"][idx])\n","    if len(trajectories) >= 5000:\n","      break"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QZeDGzT0qllK"},"outputs":[],"source":["print(len(trajectories))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oOg1ujvwBqCQ"},"outputs":[],"source":["import json\n","path = \"/content/drive/MyDrive/code/PGRainbow/valuefunctiondist.txt\"\n","np.save(path, np.array(trajectories))"]},{"cell_type":"code","source":["import json\n","import numpy as np\n","import matplotlib.pyplot as plt\n","path = \"/content/drive/MyDrive/bctp/PGRainbow/Distribution_Results/valuefunctiondist.txt.npy\"\n","traj = np.load(path, allow_pickle=True)\n","plt.hist(traj, bins=100, edgecolor='black')\n","plt.title('Histogram of V(s)')\n","plt.xlabel('Value')\n","plt.ylabel('Frequency')\n","plt.show()"],"metadata":{"id":"klU_taVcJj5z","colab":{"base_uri":"https://localhost:8080/","height":472},"executionInfo":{"status":"ok","timestamp":1721352720657,"user_tz":-540,"elapsed":2207,"user":{"displayName":"­전우재 / 학생 / 자유전공학부","userId":"15209413021938568538"}},"outputId":"1b078f70-411e-4f2b-b954-5f28a0fd7619"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 640x480 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dG30Xoqkqh2S"},"outputs":[],"source":["# Save model\n","def save(agent, optimizer, trajectories):\n","  save_path = f\"/content/drive/MyDrive/code/PGRainbow/DemonAttack/{run_name}\" + \"_model.pth\"\n","  print(f\"Saving Model to {save_path}\")\n","  torch.save({\n","          'model_state_dict': agent.state_dict(),\n","          'optimizer_state_dict': optimizer.state_dict(),\n","          'value_function': trajectories,\n","          }, save_path)\n","save(agent, optimizer, trajectories)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pgnpE_elmsQ-"},"outputs":[],"source":["\"\"\"Q(s,a)\"\"\"\n","def get_qfunc_distribution(fixed_action):\n","  args = PPOArgs\n","  # TRY NOT TO MODIFY: seeding\n","  random.seed(args.seed)\n","  np.random.seed(args.seed)\n","  torch.manual_seed(args.seed)\n","  torch.backends.cudnn.deterministic = args.torch_deterministic\n","\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n","  env_name = \"DemonAttack-v5\"\n","  envs = envpool.make(\n","      env_name,\n","      env_type=\"gym\",\n","      num_envs=args.num_envs,\n","      episodic_life=True,\n","      reward_clip=True,\n","      seed=args.seed,\n","  )\n","  envs.num_envs = args.num_envs\n","  envs.single_action_space = envs.action_space\n","  envs.single_observation_space = envs.observation_space\n","  envs = RecordEpisodeStatistics(envs)\n","\n","  \"\"\"Q(s_0, a)\"\"\"\n","  trajectories = []\n","  while len(trajectories) != 5000:\n","    obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n","    actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n","    logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n","    rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n","    dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n","    values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n","    avg_returns = deque(maxlen=20)\n","    # TRY NOT TO MODIFY: start the game\n","    global_step = 0\n","    start_time = time.time()\n","    next_obs = torch.Tensor(envs.reset()).to(device)\n","    next_done = torch.zeros(args.num_envs).to(device)\n","\n","    for step in range(0, 128):\n","      global_step += args.num_envs\n","      obs[step] = next_obs\n","      dones[step] = next_done\n","      logprob = None\n","      # ALGO LOGIC: action logic\n","      with torch.no_grad():\n","          if step != 0:\n","            action, logprob, _, value = agent.get_action_and_value(next_obs)\n","            values[step] = value.flatten()\n","          else:\n","            action = fixed_action\n","      actions[step] = action\n","      #logprobs[step] = logprob\n","\n","      # TRY NOT TO MODIFY: execute the game and log data.\n","      next_obs, reward, next_done, info = envs.step(action.cpu().numpy())\n","      rewards[step] = torch.tensor(reward).to(device).view(-1)\n","      next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)\n","\n","      for idx, d in enumerate(next_done):\n","          if d and info[\"lives\"][idx] == 0:\n","              #print(f\"global_step={global_step}, episodic_return={info['r'][idx]}\")\n","              avg_returns.append(info[\"r\"][idx])\n","              trajectories.append(info[\"r\"][idx])\n","              tqdm.write(f\"Array length: {len(trajectories)}\")\n","      if len(trajectories) >= 5000:\n","        break\n","  return trajectories"]},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","action = torch.Tensor([2,2,2,2,2,2,2,2]).to(torch.int).to(device)\n","name = \"action2\"\n","trajectories = get_qfunc_distribution(action)\n","path = f\"/content/drive/MyDrive/code/PGRainbow/{name}dist.txt\"\n","np.save(path, np.array(trajectories))\n","print(f\"Saved to {path}\")"],"metadata":{"collapsed":true,"id":"HBciQ6zjL9KN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# Create two 1D NumPy arrays (for demonstration)\n","v_path =  \"/content/drive/MyDrive/code/PGRainbow/valuefunctiondist.txt.npy\"\n","array1 = np.load(v_path, allow_pickle=True)\n","a_path = \"/content/drive/MyDrive/code/PGRainbow/action0dist.txt.npy\"\n","array2 = np.load(a_path, allow_pickle=True)\n","\n","num_bins = 50\n","\n","# Calculate the histograms\n","hist1, bins1 = np.histogram(array1, bins=num_bins)\n","hist2, bins2 = np.histogram(array2, bins=num_bins)\n","\n","# Create subplots\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n","\n","# Plot histogram 1\n","ax1.hist(array1, bins=num_bins, edgecolor='black', alpha=0.7, color='skyblue')\n","ax1.set_title('Distribution of Returns for Value Function')\n","ax1.set_xlabel('Return Values')\n","ax1.set_ylabel('Frequency')\n","ax1.grid(True)\n","\n","# Calculate and plot the mean line for histogram 1\n","mean_value1 = np.mean(array1)\n","ax1.axvline(mean_value1, color='red', linestyle='dashed', linewidth=1.5)\n","ax1.text(mean_value1 + 0.05, max(hist1) * 0.9, f'Mean: {mean_value1:.2f}', color='red')\n","\n","# Plot histogram 2\n","ax2.hist(array2, bins=num_bins, edgecolor='black', alpha=0.7, color='salmon')\n","ax2.set_title('Distribution of Returns for Action NO-OP')\n","ax2.set_xlabel('Return Values')\n","ax2.set_ylabel('Frequency')\n","ax2.grid(True)\n","\n","# Calculate and plot the mean line for histogram 2\n","mean_value2 = np.mean(array2)\n","ax2.axvline(mean_value2, color='blue', linestyle='dashed', linewidth=1.5)\n","ax2.text(mean_value2 + 0.05, max(hist2) * 0.9, f'Mean: {mean_value2:.2f}', color='blue')\n","\n","# Show the plot\n","plt.tight_layout()\n","plt.show()\n"],"metadata":{"id":"HNlvXpJ9Rat0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"kGqgjNJAoQGQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Uy9MljM1pzOL"},"outputs":[],"source":["action0 = torch.Tensor([0,0,0,0,0,0,0,0]).to(torch.int).to(device)\n","action1 = torch.Tensor([1,1,1,1,1,1,1,1]).to(torch.int).to(device)\n","action2 = torch.Tensor([2,2,2,2,2,2,2,2]).to(torch.int).to(device)\n","action3 = torch.Tensor([3,3,3,3,3,3,3,3]).to(torch.int).to(device)\n","action4 = torch.Tensor([4,4,4,4,4,4,4,4]).to(torch.int).to(device)\n","action5 = torch.Tensor([5,5,5,5,5,5,5,5]).to(torch.int).to(device)\n","items = [action0, action1, action2, action3, action4, action5]\n","\n","for item in items:\n","  print(f\"Action {item}\")\n","  name = \"action\" + str(item[0].item())\n","  trajectories = get_qfunc_distribution(item)\n","  path = f\"/content/drive/MyDrive/code/PGRainbow/{name}dist.txt\"\n","  np.save(path, np.array(trajectories))\n","  print(f\"Saved to {path}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n45-totSBezk"},"outputs":[],"source":["state = next_obs[0].unsqueeze(0)\n","fixed_state = state.repeat(args.num_envs, 1, 1, 1)\n","print(f\"fixed_state shape: {fixed_state.shape}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0Zj5EeYlSNmI"},"outputs":[],"source":["\"\"\" calculate V(s_0) and Q(s_0, a_i) and compare the distributions \"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9tG01LfsHRaf"},"outputs":[],"source":["from collections import Counter\n","\n","def get_qs_distribution(actions):\n","  rewards = np.array([])\n","  for i in range(100):\n","    _, reward, _, _ = envs.step(actions.cpu().numpy())\n","    if rewards.size == 0:\n","      rewards = reward\n","    else:\n","      # If array1 is not empty, append array2 to it\n","      rewards = np.append(rewards, reward)\n","\n","  counts = Counter(rewards)\n","  num_unique_values = len(counts)\n","  print(\"Number of unique values:\", num_unique_values)\n","  print(\"Unique values and their counts:\", counts)\n","  return counts\n","get_qs_distribution(actions)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bo-f0NFGHkHF"},"outputs":[],"source":["action0 = torch.Tensor([0,0,0,0,0,0,0,0]).to(torch.int).to(device)\n","action1 = torch.Tensor([1,1,1,1,1,1,1,1]).to(torch.int).to(device)\n","action2 = torch.Tensor([2,2,2,2,2,2,2,2]).to(torch.int).to(device)\n","action3 = torch.Tensor([3,3,3,3,3,3,3,3]).to(torch.int).to(device)\n","action4 = torch.Tensor([4,4,4,4,4,4,4,4]).to(torch.int).to(device)\n","action5 = torch.Tensor([5,5,5,5,5,5,5,5]).to(torch.int).to(device)\n","items = [action0, action1, action2, action3, action4, action5]\n","counters = {}\n","for item in items:\n","  name = \"action\" + str(item[0].item())\n","  counters[name] = get_qs_distribution(item)"]},{"cell_type":"markdown","metadata":{"id":"KbsJibH6mfpP"},"source":["## IQNPPO Train"]},{"cell_type":"markdown","metadata":{"id":"9ZuPCNkVS9Ul"},"source":["#### No lag"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HhSiwm9BCZYh"},"outputs":[],"source":["@dataclass\n","class IQNArgs:\n","    exp_name: str = \"test8_run1\"\n","    \"\"\"the name of this experiment\"\"\"\n","    seed: int = 1\n","    \"\"\"seed of the experiment\"\"\"\n","    torch_deterministic: bool = True\n","    \"\"\"if toggled, `torch.backends.cudnn.deterministic=False`\"\"\"\n","    cuda: bool = True\n","    \"\"\"if toggled, cuda will be enabled by default\"\"\"\n","    track: bool = False\n","    \"\"\"if toggled, this experiment will be tracked with Weights and Biases\"\"\"\n","    wandb_project_name: str = \"cleanRL\"\n","    \"\"\"the wandb's project name\"\"\"\n","    wandb_entity: str = None\n","    \"\"\"the entity (team) of wandb's project\"\"\"\n","    capture_video: bool = False\n","    \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n","\n","    # Algorithm specific arguments\n","    env_id: str = \"SpaceInvaders-v5\"\n","    \"\"\"the id of the environment\"\"\"\n","    total_timesteps: int = 1000000\n","    \"\"\"total timesteps of the experiments\"\"\"\n","    learning_rate: float = 2.5e-4\n","    \"\"\"the learning rate of the optimizer\"\"\"\n","    num_envs: int = 8\n","    \"\"\"the number of parallel game environments\"\"\"\n","    num_steps: int = 128\n","    \"\"\"the number of steps to run in each environment per policy rollout\"\"\"\n","    anneal_lr: bool = True\n","    \"\"\"Toggle learning rate annealing for policy and value networks\"\"\"\n","    gamma: float = 0.99\n","    \"\"\"the discount factor gamma\"\"\"\n","    gae_lambda: float = 0.95\n","    \"\"\"the lambda for the general advantage estimation\"\"\"\n","    num_minibatches: int = 4\n","    \"\"\"the number of mini-batches\"\"\"\n","    update_epochs: int = 4\n","    \"\"\"the K epochs to update the policy\"\"\"\n","    iqn_update_epochs: int = 1\n","    \"\"\"the K epochs to update the iqn\"\"\"\n","    norm_adv: bool = True\n","    \"\"\"Toggles advantages normalization\"\"\"\n","    clip_coef: float = 0.1\n","    \"\"\"the surrogate clipping coefficient\"\"\"\n","    clip_vloss: bool = True\n","    \"\"\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\"\"\"\n","    ent_coef: float = 0.01\n","    \"\"\"coefficient of the entropy\"\"\"\n","    vf_coef: float = 0.5\n","    \"\"\"coefficient of the value function\"\"\"\n","    max_grad_norm: float = 0.5\n","    \"\"\"the maximum norm for the gradient clipping\"\"\"\n","    target_kl: float = None\n","    \"\"\"the target KL divergence threshold\"\"\"\n","    iqn_start: int = 1000000 // 2\n","    iqn_per: bool = False\n","    iqn_N: int = 32\n","    double_q_learning: bool = False\n","    dueling_net: bool = False\n","    noisy_net: bool = False\n","\n","    # to be filled in runtime\n","    batch_size: int = 128*8 # args.num_envs * args.num_steps\n","    \"\"\"the batch size (computed in runtime)\"\"\"\n","    minibatch_size: int = 128//4 # args.batch_size // args.num_minibatches\n","    \"\"\"the mini-batch size (computed in runtime)\"\"\"\n","    num_iterations: int = 1000000 // (128*8) # args.total_timesteps // args.batch_size\n","    \"\"\"the number of iterations (computed in runtime)\"\"\""]},{"cell_type":"code","source":["IQNArgs.env_id"],"metadata":{"id":"qqy1gwwcLI01"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in range(0,5):\n","  args = IQNArgs\n","  args.env_id = f\"SpaceInvaders-v{i}\"\n","  print(f\"env_id: {args.env_id}\")"],"metadata":{"id":"BZOyU1viLPKZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ILIPek02IPN7","collapsed":true},"outputs":[],"source":["args = IQNArgs\n","run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}\"\n","print(f\"run_name: {run_name}\")\n","\n","writer = SummaryWriter(f\"/content/drive/MyDrive/code/PGRainbow/SpaceInvaders/{run_name}\")\n","writer.add_text(\n","    \"hyperparameters\",\n","    \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",")\n","\n","# TRY NOT TO MODIFY: seeding\n","random.seed(args.seed)\n","np.random.seed(args.seed)\n","torch.manual_seed(args.seed)\n","torch.backends.cudnn.deterministic = args.torch_deterministic\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n","\n","# env setup\n","envs = envpool.make(\n","    args.env_id,\n","    env_type=\"gym\",\n","    num_envs=args.num_envs,\n","    episodic_life=True,\n","    reward_clip=True,\n","    seed=args.seed,\n",")\n","envs.num_envs = args.num_envs\n","envs.single_action_space = envs.action_space\n","envs.single_observation_space = envs.observation_space\n","envs = RecordEpisodeStatistics(envs)\n","assert isinstance(envs.action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n","\n","iqn_configs = {\n","        'observation_space': envs.observation_space,\n","        'action_space': envs.action_space,\n","        'N': args.iqn_N,\n","        'N_dash': args.iqn_N,\n","        'double_q_learning': args.double_q_learning,\n","        'dueling_net': args.dueling_net,\n","        'noisy_net': args.noisy_net,\n","        'use_per': args.iqn_per\n","}\n","\n","iqn_model = IQNAgent(**iqn_configs)\n","iqn_optimizer = iqn_model.optimizer\n","\n","if args.iqn_per:\n","    iqn_replay_buffer = LazyPrioritizedMultiStepMemory(capacity=10**5, state_shape=envs.observation_space.shape,\n","                                                       device=device, gamma=0.99, multi_step=1, beta_steps=args.total_timesteps / 4)\n","else:\n","    iqn_replay_buffer = LazyMultiStepMemory(capacity=10**5, state_shape=envs.observation_space.shape, device=device,\n","                                            gamma=0.99, multi_step=1)\n","\n","\n","\n","agent = AgentCriticTDDistillation(envs.action_space.n, iqn_configs['N']).to(device)\n","optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n","\n","# ALGO Logic: Storage setup\n","obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n","actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n","logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n","rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n","dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n","values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n","avg_returns = deque(maxlen=20)\n","\n","# TRY NOT TO MODIFY: start the game\n","global_step = 0\n","start_time = time.time()\n","next_obs = torch.Tensor(envs.reset()).to(device)\n","next_done = torch.zeros(args.num_envs).to(device)\n","\n","loop_range = tqdm.tqdm(range(0, args.num_iterations + 1))\n","for iteration in loop_range:\n","    # Annealing the rate if instructed to do so.\n","    if args.anneal_lr:\n","        frac = 1.0 - (iteration - 1.0) / args.num_iterations\n","        lrnow = frac * args.learning_rate\n","        optimizer.param_groups[0][\"lr\"] = lrnow\n","\n","    for step in range(0, args.num_steps):\n","        global_step += args.num_envs\n","        obs[step] = next_obs\n","        dones[step] = next_done\n","\n","        # ALGO LOGIC: action logic\n","        with torch.no_grad():\n","            q, quantiles = iqn_model.online_net.calculate_q(torch.tensor(next_obs, dtype=torch.float32).to(device))\n","            action, logprob, _, value = agent(torch.tensor(next_obs, dtype=torch.float32).to(device), q)\n","            #print(f\"ALGO LOGIC: action logic action: {action.shape}\")\n","            #print(f\"ALGO LOGIC: action logic value: {value.shape}\")\n","            values[step] = value.flatten()\n","            #print(f\"ALGO LOGIC: action logic values shape: {values.shape}\")\n","        actions[step] = action\n","        #print(f\"ALGO LOGIC: action logic actions shape: {actions.shape}\")\n","        logprobs[step] = logprob\n","        #print(f\"ALGO LOGIC: action logic logprobs shape: {logprobs.shape}\")\n","\n","        # TRY NOT TO MODIFY: execute the game and log data.\n","        state = next_obs\n","        next_obs, reward, next_done, info = envs.step(action.cpu().numpy())\n","        rewards[step] = torch.tensor(reward).to(device).view(-1)\n","        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)\n","\n","        ################## Fill IQN Replay Buffer ##################\n","        experience_batch = {\n","                        'obs': state,\n","                        'next_obs': next_obs,\n","                        'actions': action,\n","                        'rewards': reward,\n","                        'dones': next_done,\n","                        'infos': info\n","        }\n","        s, ns, ac, re, do, inf = experience_batch['obs'], experience_batch['next_obs'], experience_batch['actions'], experience_batch['rewards'], experience_batch['dones'], experience_batch['infos']\n","        for w in range(args.num_envs):\n","          o = s[w].cpu()\n","          # print(f\"o type: {type(o)}\")\n","          #print(f\"o shape: {o.shape}\")\n","          no = ns[w].cpu()\n","          #print(f\"no shape: {no.shape}\")\n","          a = ac[w].detach().cpu().numpy()\n","          #print(f\"a shape: {a} and type: {type(a)}\")\n","          r = re[w]\n","          #print(f\"r shape: {r}\")\n","          d = do[w].cpu()\n","          #print(f\"d shape: {d}\")\n","          i = inf\n","          iqn_replay_buffer.append(o, a, r, no, d)\n","        ########################################################################\n","\n","        for idx, d in enumerate(next_done):\n","            if d and info[\"lives\"][idx] == 0:\n","                print(f\"global_step={global_step}, episodic_return={info['r'][idx]}\")\n","                avg_returns.append(info[\"r\"][idx])\n","                writer.add_scalar(\"charts/avg_episodic_return\", np.average(avg_returns), global_step)\n","                writer.add_scalar(\"charts/episodic_return\", info[\"r\"][idx], global_step)\n","                writer.add_scalar(\"charts/episodic_length\", info[\"l\"][idx], global_step)\n","\n","    # bootstrap value if not done\n","    with torch.no_grad():\n","        #print(f\"next_obs shape: {next_obs.shape}\")\n","        q, quantiles = iqn_model.online_net.calculate_q(torch.tensor(next_obs, dtype=torch.float32).to(device))\n","        _, _, _, next_value = agent(torch.tensor(next_obs, dtype=torch.float32).to(device), q)\n","        next_value = next_value.reshape(1,-1)\n","        #print(f\"gae next_value shape: {next_value.shape}\")\n","        #print(f\"rewards shape: {rewards.shape}\")\n","        advantages = torch.zeros_like(rewards).to(device)\n","        #print(f\"gae advantages shape: {advantages.shape}\")\n","        #print(f\"gae values shape: {values.shape}\")\n","        lastgaelam = 0\n","        for t in reversed(range(args.num_steps)):\n","            if t == args.num_steps - 1:\n","                nextnonterminal = 1.0 - next_done\n","                nextvalues = next_value\n","            else:\n","                nextnonterminal = 1.0 - dones[t + 1]\n","                nextvalues = values[t + 1]\n","            delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n","            #print(f\"delta shape: {delta.shape}\")\n","            advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n","            #print(f\"advantages shape: {advantages.shape}\")\n","        returns = advantages + values\n","\n","    # flatten the batch\n","    b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n","    b_logprobs = logprobs.reshape(-1)\n","    b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n","    b_advantages = advantages.reshape(-1)\n","    b_returns = returns.reshape(-1)\n","    b_values = values.reshape(-1)\n","\n","    ################# IQN Train #################\n","    if len(iqn_replay_buffer) >= 32:\n","        cum_iqn_loss = 0\n","        for iqn_update_epoch in range(args.iqn_update_epochs):\n","          if args.iqn_per:\n","            (iqn_states, iqn_actions, iqn_rewards, iqn_next_states, iqn_dones), iqn_weights = iqn_replay_buffer.sample(32)\n","          else:\n","            iqn_states, iqn_actions, iqn_rewards, iqn_next_states, iqn_dones = iqn_replay_buffer.sample(32)\n","          iqn_loss, errors = iqn_model.train(iqn_states, iqn_actions, iqn_rewards, iqn_next_states, iqn_dones)\n","          if args.iqn_per:\n","              iqn_replay_buffer.update_priority(errors)\n","          cum_iqn_loss += iqn_loss\n","        writer.add_scalar(\"train/iqn_loss\", cum_iqn_loss, global_step)\n","    if iteration % 250 == 0:\n","        print(f\"Updating target iqn model at step: {iteration}\")\n","        iqn_model.update_target()\n","    ###################################################\n","\n","    # Optimizing the policy and value network\n","    b_inds = np.arange(args.batch_size)\n","    clipfracs = []\n","    for epoch in range(args.update_epochs):\n","        np.random.shuffle(b_inds)\n","        for start in range(0, args.batch_size, args.minibatch_size):\n","            end = start + args.minibatch_size\n","            mb_inds = b_inds[start:end]\n","\n","            q, quantiles = iqn_model.online_net.calculate_q(torch.tensor(b_obs[mb_inds], dtype=torch.float32).to(device))\n","            #print(f\"b_obs[mb_inds] shape: {b_obs[mb_inds].shape} q shape: {q.shape}\")\n","            #print(f\"b_actions.long()[mb_inds] shape: {b_actions.long()[mb_inds].shape}\")\n","            #print(b_obs[mb_inds])\n","            _, newlogprob, entropy, newvalue = agent(torch.tensor(b_obs[mb_inds], dtype=torch.float32).to(device), q, action=b_actions.long()[mb_inds])\n","            logratio = newlogprob - b_logprobs[mb_inds]\n","            ratio = logratio.exp()\n","\n","            with torch.no_grad():\n","                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n","                old_approx_kl = (-logratio).mean()\n","                approx_kl = ((ratio - 1) - logratio).mean()\n","                clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n","\n","            mb_advantages = b_advantages[mb_inds]\n","            if args.norm_adv:\n","                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n","\n","            # Policy loss\n","            pg_loss1 = -mb_advantages * ratio\n","            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n","            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n","\n","            # Value loss\n","            newvalue = newvalue.view(-1)\n","            if args.clip_vloss:\n","                v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n","                v_clipped = b_values[mb_inds] + torch.clamp(\n","                    newvalue - b_values[mb_inds],\n","                    -args.clip_coef,\n","                    args.clip_coef,\n","                )\n","                v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n","                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n","                v_loss = 0.5 * v_loss_max.mean()\n","            else:\n","                v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n","\n","            entropy_loss = entropy.mean()\n","            loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n","            optimizer.step()\n","\n","        if args.target_kl is not None and approx_kl > args.target_kl:\n","            break\n","\n","    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n","    var_y = np.var(y_true)\n","    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n","\n","    # TRY NOT TO MODIFY: record rewards for plotting purposes\n","    writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n","    writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n","    writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n","    writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n","    writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n","    writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n","    writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n","    writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n","    print(\"SPS:\", int(global_step / (time.time() - start_time)))\n","    writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n","\n","envs.close()\n","writer.close()"]},{"cell_type":"markdown","source":["#### Value Function Distribution"],"metadata":{"id":"oafJCg3QEzSj"}},{"cell_type":"code","source":["@dataclass\n","class DistIQNArgs:\n","    exp_name: str = \"iqn_ppo_critic_cat_run1\"\n","    \"\"\"the name of this experiment\"\"\"\n","    seed: int = 1\n","    \"\"\"seed of the experiment\"\"\"\n","    torch_deterministic: bool = True\n","    \"\"\"if toggled, `torch.backends.cudnn.deterministic=False`\"\"\"\n","    cuda: bool = True\n","    \"\"\"if toggled, cuda will be enabled by default\"\"\"\n","    track: bool = False\n","    \"\"\"if toggled, this experiment will be tracked with Weights and Biases\"\"\"\n","    wandb_project_name: str = \"cleanRL\"\n","    \"\"\"the wandb's project name\"\"\"\n","    wandb_entity: str = None\n","    \"\"\"the entity (team) of wandb's project\"\"\"\n","    capture_video: bool = False\n","    \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n","\n","    # Algorithm specific arguments\n","    env_id: str = \"DemonAttack-v5\"\n","    \"\"\"the id of the environment\"\"\"\n","    total_timesteps: int = 1000000\n","    \"\"\"total timesteps of the experiments\"\"\"\n","    learning_rate: float = 2.5e-4\n","    \"\"\"the learning rate of the optimizer\"\"\"\n","    num_envs: int = 8\n","    \"\"\"the number of parallel game environments\"\"\"\n","    num_steps: int = 128\n","    \"\"\"the number of steps to run in each environment per policy rollout\"\"\"\n","    anneal_lr: bool = True\n","    \"\"\"Toggle learning rate annealing for policy and value networks\"\"\"\n","    gamma: float = 0.99\n","    \"\"\"the discount factor gamma\"\"\"\n","    gae_lambda: float = 0.95\n","    \"\"\"the lambda for the general advantage estimation\"\"\"\n","    num_minibatches: int = 4\n","    \"\"\"the number of mini-batches\"\"\"\n","    update_epochs: int = 4\n","    \"\"\"the K epochs to update the policy\"\"\"\n","    iqn_update_epochs: int = 1\n","    \"\"\"the K epochs to update the iqn\"\"\"\n","    norm_adv: bool = True\n","    \"\"\"Toggles advantages normalization\"\"\"\n","    clip_coef: float = 0.1\n","    \"\"\"the surrogate clipping coefficient\"\"\"\n","    clip_vloss: bool = True\n","    \"\"\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\"\"\"\n","    ent_coef: float = 0.01\n","    \"\"\"coefficient of the entropy\"\"\"\n","    vf_coef: float = 0.5\n","    \"\"\"coefficient of the value function\"\"\"\n","    max_grad_norm: float = 0.5\n","    \"\"\"the maximum norm for the gradient clipping\"\"\"\n","    target_kl: float = None\n","    \"\"\"the target KL divergence threshold\"\"\"\n","    iqn_start: int = 1000000 // 2\n","    iqn_per: bool = False\n","    iqn_N: int = 32\n","    double_q_learning: bool = False\n","    dueling_net: bool = False\n","    noisy_net: bool = False\n","\n","    # to be filled in runtime\n","    batch_size: int = 128*8 # args.num_envs * args.num_steps\n","    \"\"\"the batch size (computed in runtime)\"\"\"\n","    minibatch_size: int = 128//4 # args.batch_size // args.num_minibatches\n","    \"\"\"the mini-batch size (computed in runtime)\"\"\"\n","    num_iterations: int = 1000000 // (128*8) # args.total_timesteps // args.batch_size\n","    \"\"\"the number of iterations (computed in runtime)\"\"\""],"metadata":{"id":"qyrz6bZwEx33"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"V(s_0) PG-RAINBOW\"\"\"\n","args = DistIQNArgs\n","# TRY NOT TO MODIFY: seeding\n","random.seed(args.seed)\n","np.random.seed(args.seed)\n","torch.manual_seed(args.seed)\n","torch.backends.cudnn.deterministic = args.torch_deterministic\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n","\n","env_name = \"DemonAttack-v5\"\n","envs = envpool.make(\n","    env_name,\n","    env_type=\"gym\",\n","    num_envs=args.num_envs,\n","    episodic_life=True,\n","    reward_clip=True,\n","    seed=args.seed,\n",")\n","envs.num_envs = args.num_envs\n","envs.single_action_space = envs.action_space\n","envs.single_observation_space = envs.observation_space\n","envs = RecordEpisodeStatistics(envs)\n","\n","iqn_configs = {\n","        'observation_space': envs.observation_space,\n","        'action_space': envs.action_space,\n","        'N': args.iqn_N,\n","        'N_dash': args.iqn_N,\n","        'double_q_learning': args.double_q_learning,\n","        'dueling_net': args.dueling_net,\n","        'noisy_net': args.noisy_net,\n","        'use_per': args.iqn_per\n","}\n","\n","iqn_model = IQNAgent(**iqn_configs)\n","iqn_optimizer = iqn_model.optimizer\n","\n","if args.iqn_per:\n","    iqn_replay_buffer = LazyPrioritizedMultiStepMemory(capacity=10**5, state_shape=envs.observation_space.shape,\n","                                                       device=device, gamma=0.99, multi_step=1, beta_steps=args.total_timesteps / 4)\n","else:\n","    iqn_replay_buffer = LazyMultiStepMemory(capacity=10**5, state_shape=envs.observation_space.shape, device=device,\n","                                            gamma=0.99, multi_step=1)\n","\n","\n","\n","agent = AgentCriticDistillation(envs.action_space.n, iqn_configs['N']).to(device)\n","optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n","\n","obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n","actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n","logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n","rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n","dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n","values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n","avg_returns = deque(maxlen=20)\n","# TRY NOT TO MODIFY: start the game\n","global_step = 0\n","start_time = time.time()\n","next_obs = torch.Tensor(envs.reset()).to(device)\n","next_done = torch.zeros(args.num_envs).to(device)\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","trajectories = []\n","while len(trajectories) != 5000:\n","  for step in range(0, args.num_steps):\n","    global_step += args.num_envs\n","    obs[step] = next_obs\n","    dones[step] = next_done\n","\n","    # ALGO LOGIC: action logic\n","    with torch.no_grad():\n","        q, quantiles = iqn_model.online_net.calculate_q(torch.tensor(next_obs, dtype=torch.float32).to(device))\n","        action, logprob, _, value = agent(torch.tensor(next_obs, dtype=torch.float32).to(device), q)\n","    actions[step] = action\n","    logprobs[step] = logprob\n","\n","    # TRY NOT TO MODIFY: execute the game and log data.\n","    state = next_obs\n","    next_obs, reward, next_done, info = envs.step(action.cpu().numpy())\n","    rewards[step] = torch.tensor(reward).to(device).view(-1)\n","    next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)\n","\n","    ################## Fill IQN Replay Buffer ##################\n","    experience_batch = {\n","                    'obs': state,\n","                    'next_obs': next_obs,\n","                    'actions': action,\n","                    'rewards': reward,\n","                    'dones': next_done,\n","                    'infos': info\n","    }\n","    s, ns, ac, re, do, inf = experience_batch['obs'], experience_batch['next_obs'], experience_batch['actions'], experience_batch['rewards'], experience_batch['dones'], experience_batch['infos']\n","    for w in range(args.num_envs):\n","      o = s[w].cpu()\n","      # print(f\"o type: {type(o)}\")\n","      #print(f\"o shape: {o.shape}\")\n","      no = ns[w].cpu()\n","      #print(f\"no shape: {no.shape}\")\n","      a = ac[w].detach().cpu().numpy()\n","      #print(f\"a shape: {a} and type: {type(a)}\")\n","      r = re[w]\n","      #print(f\"r shape: {r}\")\n","      d = do[w].cpu()\n","      #print(f\"d shape: {d}\")\n","      i = inf\n","      iqn_replay_buffer.append(o, a, r, no, d)\n","    ########################################################################\n","    for idx, d in enumerate(next_done):\n","        if d and info[\"lives\"][idx] == 0:\n","            print(f\"global_step={global_step}, episodic_return={info['r'][idx]}\")\n","            avg_returns.append(info[\"r\"][idx])\n","            trajectories.append(info[\"r\"][idx])\n","            tqdm.write(f\"Array length: {len(trajectories)}\")\n","    if len(trajectories) >= 5000:\n","      break"],"metadata":{"id":"egy-f1o7Dc7C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CJFGhKowIVan"},"source":["#### Lagging Trainer (Bad Performance)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vkggsy-Yb_Fj"},"outputs":[],"source":["@dataclass\n","class LagIQNArgs:\n","    exp_name: str = \"iqn_ppo_lagcritic_run2\"\n","    \"\"\"the name of this experiment\"\"\"\n","    seed: int = 1\n","    \"\"\"seed of the experiment\"\"\"\n","    torch_deterministic: bool = True\n","    \"\"\"if toggled, `torch.backends.cudnn.deterministic=False`\"\"\"\n","    cuda: bool = True\n","    \"\"\"if toggled, cuda will be enabled by default\"\"\"\n","    track: bool = False\n","    \"\"\"if toggled, this experiment will be tracked with Weights and Biases\"\"\"\n","    wandb_project_name: str = \"cleanRL\"\n","    \"\"\"the wandb's project name\"\"\"\n","    wandb_entity: str = None\n","    \"\"\"the entity (team) of wandb's project\"\"\"\n","    capture_video: bool = False\n","    \"\"\"whether to capture videos of the agent performances (check out `videos` folder)\"\"\"\n","\n","    # Algorithm specific arguments\n","    env_id: str = \"AirRaid-v5\"\n","    \"\"\"the id of the environment\"\"\"\n","    total_timesteps: int = 1000000\n","    \"\"\"total timesteps of the experiments\"\"\"\n","    learning_rate: float = 2.5e-4\n","    \"\"\"the learning rate of the optimizer\"\"\"\n","    num_envs: int = 8\n","    \"\"\"the number of parallel game environments\"\"\"\n","    num_steps: int = 128\n","    \"\"\"the number of steps to run in each environment per policy rollout\"\"\"\n","    anneal_lr: bool = True\n","    \"\"\"Toggle learning rate annealing for policy and value networks\"\"\"\n","    gamma: float = 0.99\n","    \"\"\"the discount factor gamma\"\"\"\n","    gae_lambda: float = 0.95\n","    \"\"\"the lambda for the general advantage estimation\"\"\"\n","    num_minibatches: int = 4\n","    \"\"\"the number of mini-batches\"\"\"\n","    update_epochs: int = 4\n","    \"\"\"the K epochs to update the policy\"\"\"\n","    norm_adv: bool = True\n","    \"\"\"Toggles advantages normalization\"\"\"\n","    clip_coef: float = 0.1\n","    \"\"\"the surrogate clipping coefficient\"\"\"\n","    clip_vloss: bool = True\n","    \"\"\"Toggles whether or not to use a clipped loss for the value function, as per the paper.\"\"\"\n","    ent_coef: float = 0.01\n","    \"\"\"coefficient of the entropy\"\"\"\n","    vf_coef: float = 0.5\n","    \"\"\"coefficient of the value function\"\"\"\n","    max_grad_norm: float = 0.5\n","    \"\"\"the maximum norm for the gradient clipping\"\"\"\n","    target_kl: float = None\n","    \"\"\"the target KL divergence threshold\"\"\"\n","    iqn_start: int = 1000000 // 2\n","    iqn_per: bool = False\n","    iqn_N: int = 32\n","    double_q_learning: bool = False\n","    dueling_net: bool = False\n","    noisy_net: bool = False\n","\n","    # to be filled in runtime\n","    batch_size: int = 128*8 # args.num_envs * args.num_steps\n","    \"\"\"the batch size (computed in runtime)\"\"\"\n","    minibatch_size: int = 128//4 # args.batch_size // args.num_minibatches\n","    \"\"\"the mini-batch size (computed in runtime)\"\"\"\n","    num_iterations: int = 1000000 // (128*8) # args.total_timesteps // args.batch_size\n","    \"\"\"the number of iterations (computed in runtime)\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XuKtC7Bjml0p"},"outputs":[],"source":["#################################### Lagging Distillation ####################################\n","\n","args = LagIQNArgs\n","run_name = f\"{args.env_id}__{args.exp_name}__{args.seed}\"\n","\n","writer = SummaryWriter(f\"/content/drive/MyDrive/code/PGRainbow/AirRaid/{run_name}\")\n","writer.add_text(\n","    \"hyperparameters\",\n","    \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(args).items()])),\n",")\n","\n","# TRY NOT TO MODIFY: seeding\n","random.seed(args.seed)\n","np.random.seed(args.seed)\n","torch.manual_seed(args.seed)\n","torch.backends.cudnn.deterministic = args.torch_deterministic\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n","\n","# env setup\n","envs = envpool.make(\n","    args.env_id,\n","    env_type=\"gym\",\n","    num_envs=args.num_envs,\n","    episodic_life=True,\n","    reward_clip=True,\n","    seed=args.seed,\n",")\n","envs.num_envs = args.num_envs\n","envs.single_action_space = envs.action_space\n","envs.single_observation_space = envs.observation_space\n","envs = RecordEpisodeStatistics(envs)\n","assert isinstance(envs.action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n","\n","iqn_configs = {\n","        'observation_space': envs.observation_space,\n","        'action_space': envs.action_space,\n","        'N': args.iqn_N,\n","        'N_dash': args.iqn_N,\n","        'double_q_learning': args.double_q_learning,\n","        'dueling_net': args.dueling_net,\n","        'noisy_net': args.noisy_net,\n","        'use_per': args.iqn_per\n","}\n","iqn_model = IQNAgent(**iqn_configs)\n","iqn_optimizer = iqn_model.optimizer\n","start_iqn = False\n","\n","iqn_replay_buffer = LazyMultiStepMemory(capacity=10**5, state_shape=envs.observation_space.shape, device=device, gamma=0.99, multi_step=1)\n","\n","agent = LagAgentCriticDistillation(envs.action_space.n, iqn_configs['N']).to(device)\n","optimizer = optim.Adam(agent.parameters(), lr=args.learning_rate, eps=1e-5)\n","\n","# ALGO Logic: Storage setup\n","obs = torch.zeros((args.num_steps, args.num_envs) + envs.single_observation_space.shape).to(device)\n","actions = torch.zeros((args.num_steps, args.num_envs) + envs.single_action_space.shape).to(device)\n","logprobs = torch.zeros((args.num_steps, args.num_envs)).to(device)\n","rewards = torch.zeros((args.num_steps, args.num_envs)).to(device)\n","dones = torch.zeros((args.num_steps, args.num_envs)).to(device)\n","values = torch.zeros((args.num_steps, args.num_envs)).to(device)\n","avg_returns = deque(maxlen=20)\n","\n","# TRY NOT TO MODIFY: start the game\n","global_step = 0\n","start_time = time.time()\n","next_obs = torch.Tensor(envs.reset()).to(device)\n","next_done = torch.zeros(args.num_envs).to(device)\n","\n","loop_range = tqdm.tqdm(range(0, args.num_iterations + 1))\n","for iteration in loop_range:\n","    # Start IQN Distillation\n","    if iteration > (args.num_iterations // 2):\n","      start_iqn = True\n","      print(f\"IQN Distillation Started at step {global_step}\")\n","    # Annealing the rate if instructed to do so.\n","    if args.anneal_lr:\n","        frac = 1.0 - (iteration - 1.0) / args.num_iterations\n","        lrnow = frac * args.learning_rate\n","        optimizer.param_groups[0][\"lr\"] = lrnow\n","\n","    for step in range(0, args.num_steps):\n","        global_step += args.num_envs\n","        obs[step] = next_obs\n","        dones[step] = next_done\n","\n","        # ALGO LOGIC: action logic\n","        with torch.no_grad():\n","            q, quantiles = iqn_model.online_net.calculate_q(torch.tensor(next_obs, dtype=torch.float32).to(device))\n","            action, logprob, _, value = agent(torch.tensor(next_obs, dtype=torch.float32).to(device), q, start_iqn=start_iqn)\n","            #print(f\"ALGO LOGIC: action logic action: {action.shape}\")\n","            #print(f\"ALGO LOGIC: action logic value: {value.shape}\")\n","            values[step] = value.flatten()\n","            #print(f\"ALGO LOGIC: action logic values shape: {values.shape}\")\n","        actions[step] = action\n","        #print(f\"ALGO LOGIC: action logic actions shape: {actions.shape}\")\n","        logprobs[step] = logprob\n","        #print(f\"ALGO LOGIC: action logic logprobs shape: {logprobs.shape}\")\n","\n","        # TRY NOT TO MODIFY: execute the game and log data.\n","        state = next_obs\n","        next_obs, reward, next_done, info = envs.step(action.cpu().numpy())\n","        rewards[step] = torch.tensor(reward).to(device).view(-1)\n","        next_obs, next_done = torch.Tensor(next_obs).to(device), torch.Tensor(next_done).to(device)\n","\n","        ################## Fill IQN Replay Buffer ##################\n","        experience_batch = {\n","                        'obs': state,\n","                        'next_obs': next_obs,\n","                        'actions': action,\n","                        'rewards': reward,\n","                        'dones': next_done,\n","                        'infos': info\n","        }\n","        s, ns, ac, re, do, inf = experience_batch['obs'], experience_batch['next_obs'], experience_batch['actions'], experience_batch['rewards'], experience_batch['dones'], experience_batch['infos']\n","        for w in range(args.num_envs):\n","          o = s[w].cpu()\n","          # print(f\"o type: {type(o)}\")\n","          #print(f\"o shape: {o.shape}\")\n","          no = ns[w].cpu()\n","          #print(f\"no shape: {no.shape}\")\n","          a = ac[w].detach().cpu().numpy()\n","          #print(f\"a shape: {a} and type: {type(a)}\")\n","          r = re[w]\n","          #print(f\"r shape: {r}\")\n","          d = do[w].cpu()\n","          #print(f\"d shape: {d}\")\n","          i = inf\n","          iqn_replay_buffer.append(o, a, r, no, d)\n","        ########################################################################\n","\n","        for idx, d in enumerate(next_done):\n","            if d and info[\"lives\"][idx] == 0:\n","                print(f\"global_step={global_step}, episodic_return={info['r'][idx]}\")\n","                avg_returns.append(info[\"r\"][idx])\n","                writer.add_scalar(\"charts/avg_episodic_return\", np.average(avg_returns), global_step)\n","                writer.add_scalar(\"charts/episodic_return\", info[\"r\"][idx], global_step)\n","                writer.add_scalar(\"charts/episodic_length\", info[\"l\"][idx], global_step)\n","\n","    # bootstrap value if not done\n","    with torch.no_grad():\n","        #print(f\"next_obs shape: {next_obs.shape}\")\n","        q, quantiles = iqn_model.online_net.calculate_q(torch.tensor(next_obs, dtype=torch.float32).to(device))\n","        _, _, _, next_value = agent(torch.tensor(next_obs, dtype=torch.float32).to(device), q, start_iqn=start_iqn)\n","        next_value = next_value.reshape(1,-1)\n","        #print(f\"gae next_value shape: {next_value.shape}\")\n","        #print(f\"rewards shape: {rewards.shape}\")\n","        advantages = torch.zeros_like(rewards).to(device)\n","        #print(f\"gae advantages shape: {advantages.shape}\")\n","        #print(f\"gae values shape: {values.shape}\")\n","        lastgaelam = 0\n","        for t in reversed(range(args.num_steps)):\n","            if t == args.num_steps - 1:\n","                nextnonterminal = 1.0 - next_done\n","                nextvalues = next_value\n","            else:\n","                nextnonterminal = 1.0 - dones[t + 1]\n","                nextvalues = values[t + 1]\n","            delta = rewards[t] + args.gamma * nextvalues * nextnonterminal - values[t]\n","            #print(f\"delta shape: {delta.shape}\")\n","            advantages[t] = lastgaelam = delta + args.gamma * args.gae_lambda * nextnonterminal * lastgaelam\n","            #print(f\"advantages shape: {advantages.shape}\")\n","        returns = advantages + values\n","\n","    # flatten the batch\n","    b_obs = obs.reshape((-1,) + envs.single_observation_space.shape)\n","    b_logprobs = logprobs.reshape(-1)\n","    b_actions = actions.reshape((-1,) + envs.single_action_space.shape)\n","    b_advantages = advantages.reshape(-1)\n","    b_returns = returns.reshape(-1)\n","    b_values = values.reshape(-1)\n","\n","    ################# IQN Train #################\n","    if len(iqn_replay_buffer) >= 32:\n","        cum_iqn_loss = 0\n","        iqn_states, iqn_actions, iqn_rewards, iqn_next_states, iqn_dones = iqn_replay_buffer.sample(32)\n","        iqn_loss, _ = iqn_model.train(iqn_states, iqn_actions, iqn_rewards, iqn_next_states, iqn_dones)\n","        cum_iqn_loss += iqn_loss\n","        writer.add_scalar(\"train/iqn_loss\", cum_iqn_loss, global_step)\n","    if iteration % 250 == 0:\n","        print(f\"Updating target iqn model at step: {iteration}\")\n","        iqn_model.update_target()\n","    ###################################################\n","\n","    # Optimizing the policy and value network\n","    b_inds = np.arange(args.batch_size)\n","    clipfracs = []\n","    for epoch in range(args.update_epochs):\n","        np.random.shuffle(b_inds)\n","        for start in range(0, args.batch_size, args.minibatch_size):\n","            end = start + args.minibatch_size\n","            mb_inds = b_inds[start:end]\n","\n","            q, quantiles = iqn_model.online_net.calculate_q(torch.tensor(b_obs[mb_inds], dtype=torch.float32).to(device))\n","            #print(f\"b_obs[mb_inds] shape: {b_obs[mb_inds].shape} q shape: {q.shape}\")\n","            #print(f\"b_actions.long()[mb_inds] shape: {b_actions.long()[mb_inds].shape}\")\n","            _, newlogprob, entropy, newvalue = agent(torch.tensor(b_obs[mb_inds], dtype=torch.float32).to(device), q, start_iqn=start_iqn, action=b_actions.long()[mb_inds])\n","            logratio = newlogprob - b_logprobs[mb_inds]\n","            ratio = logratio.exp()\n","\n","            with torch.no_grad():\n","                # calculate approx_kl http://joschu.net/blog/kl-approx.html\n","                old_approx_kl = (-logratio).mean()\n","                approx_kl = ((ratio - 1) - logratio).mean()\n","                clipfracs += [((ratio - 1.0).abs() > args.clip_coef).float().mean().item()]\n","\n","            mb_advantages = b_advantages[mb_inds]\n","            if args.norm_adv:\n","                mb_advantages = (mb_advantages - mb_advantages.mean()) / (mb_advantages.std() + 1e-8)\n","\n","            # Policy loss\n","            pg_loss1 = -mb_advantages * ratio\n","            pg_loss2 = -mb_advantages * torch.clamp(ratio, 1 - args.clip_coef, 1 + args.clip_coef)\n","            pg_loss = torch.max(pg_loss1, pg_loss2).mean()\n","\n","            # Value loss\n","            newvalue = newvalue.view(-1)\n","            if args.clip_vloss:\n","                v_loss_unclipped = (newvalue - b_returns[mb_inds]) ** 2\n","                v_clipped = b_values[mb_inds] + torch.clamp(\n","                    newvalue - b_values[mb_inds],\n","                    -args.clip_coef,\n","                    args.clip_coef,\n","                )\n","                v_loss_clipped = (v_clipped - b_returns[mb_inds]) ** 2\n","                v_loss_max = torch.max(v_loss_unclipped, v_loss_clipped)\n","                v_loss = 0.5 * v_loss_max.mean()\n","            else:\n","                v_loss = 0.5 * ((newvalue - b_returns[mb_inds]) ** 2).mean()\n","\n","            entropy_loss = entropy.mean()\n","            loss = pg_loss - args.ent_coef * entropy_loss + v_loss * args.vf_coef\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","            nn.utils.clip_grad_norm_(agent.parameters(), args.max_grad_norm)\n","            optimizer.step()\n","\n","        if args.target_kl is not None and approx_kl > args.target_kl:\n","            break\n","\n","    y_pred, y_true = b_values.cpu().numpy(), b_returns.cpu().numpy()\n","    var_y = np.var(y_true)\n","    explained_var = np.nan if var_y == 0 else 1 - np.var(y_true - y_pred) / var_y\n","\n","    # TRY NOT TO MODIFY: record rewards for plotting purposes\n","    writer.add_scalar(\"charts/learning_rate\", optimizer.param_groups[0][\"lr\"], global_step)\n","    writer.add_scalar(\"losses/value_loss\", v_loss.item(), global_step)\n","    writer.add_scalar(\"losses/policy_loss\", pg_loss.item(), global_step)\n","    writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n","    writer.add_scalar(\"losses/old_approx_kl\", old_approx_kl.item(), global_step)\n","    writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n","    writer.add_scalar(\"losses/clipfrac\", np.mean(clipfracs), global_step)\n","    writer.add_scalar(\"losses/explained_variance\", explained_var, global_step)\n","    print(\"SPS:\", int(global_step / (time.time() - start_time)))\n","    writer.add_scalar(\"charts/SPS\", int(global_step / (time.time() - start_time)), global_step)\n","\n","envs.close()\n","writer.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"E8OGerWiCNXK"},"outputs":[],"source":["save_path = f\"/content/drive/MyDrive/code/PGRainbow/SpaceInvaders/cleanrl/runs/{run_name}\" + \"iqnppo_model.pth\"\n","print(f\"Saving Model to {save_path}\")\n","torch.save({\n","        'model_state_dict': agent.state_dict(),\n","        'optimizer_state_dict': optimizer.state_dict(),\n","        'iqn_model_state-dict': iqn_model.state_dict(),\n","        'iqn_optimizer_state_dict': iqn_optimizer.state_dict(),\n","        }, save_path)"]},{"cell_type":"markdown","metadata":{"id":"X3hUxppwIYaW"},"source":["## Results"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X4hqQzvpbQMM"},"outputs":[],"source":["%load_ext tensorboard"]},{"cell_type":"code","source":["%tensorboard --logdir=/content/drive/MyDrive/code/PGRainbow/AirRaid"],"metadata":{"id":"8WJsSf8Z9Le9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"he0EJO-W9OKE"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"collapsed_sections":["ANBUy1TzlxN4","eSxMzLcUmx0t","PDbjQZcSmzVb","Me3-jYDATLYN","zdDQU6ygpbp8","9VvQgBfjp5zD","pBqRPr_kpEXc","8e75j4QEptLC","y3210QiPpJEd","sOTWS-NHmdQY","M14TrXHBqIMs","oafJCg3QEzSj","CJFGhKowIVan"],"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}